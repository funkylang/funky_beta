#
  Copyright (C) 2025 by
  Dipl.-Ing. Michael Niederle

  This program is free software; you can redistribute it and/or modify
  it under the terms of the GNU Library General Public License, version 2, or
  (at your option) under the terms of the GNU Lesser General Public License,
  version 3.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
  GNU Lesser (Library) General Public License for more details.

  For details of the GNU General Public License see the accompanying
  files LGPLv2.txt and LGLPv3.txt or
  http://www.gnu.org/licenses/lgpl-2.0.html
  http://www.gnu.org/licenses/lgpl-3.0.html
  or print to the
  Free Software Foundation, Inc.,
  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.

#
  Topic: AI

  Funky currently supports large language models based on the llama.cpp project.

  The "fllama" tool is a server program that must be started first before a
  client can send queries to the server.

  A short glossary:

  * An *LLM* is a Large Language Model.

  * A *modelname* is the name of an LLM in GGUF-format including the ".gguf"
    file extension.

  * A *model* is an object on the client side that represents an LLM.

  * A *token* is a non-negative integer number that represents one ore more
    (maybe partial) characters.

  * A *piece* is a raw octet string associated with a token. This string can
    contain partial UTF-8-sequences. Most models do not contain so many tokens
    to be able to encode all possible Unicode code points. So smileys might
    be represented by two tokens each describing a part of a single code point.

  * A *sequence* is a sequence of tokens stored on the server. Such a sequence
    contains prompts (supplied by the client) and answers (generated by the
    server). Sequences can be truncated, some tokens can be deleted or new
    tokens can be inserted. This might harm the quality of later evaluations
    because of rounding errors. Sequences can also be copied to share a common
    prefix string (e.g. some instructions) with each other.

  * A *sequence-id* is a client supplied non-negative integer used to identify
    a sequence.

  * *logits* are float values that describe how good a token is suited to
    continue a sequence. The higher the value the better the token represents
    the "inner state" of the model. Logit values are logarithmic, so a slightly
    higher value represents a much better match. The absolute value of logits
    are model dependent.

  * *tokenization* is the conversion of a text string into tokens.

  * *detokenization* is the conversion of tokens into a text stringl

  * *evaluation* is the generation of a list of logits to choose from for the
    next token based on the tokens already in the sequence

  Example:

    <require basic/stdlib>
    <require ai/new_llama>

    <using std>
    <using llama>

    $MODEL_NAME "sauerkrautlm-una-solar-instruct.Q5_K_M.gguf"
    $PROMPT "Once upon a time"

    $SEQ_ID 1
    open_model! $model MODEL_NAME
    tokenize! $tokens model PROMPT
    create_sequence! model SEQ_ID
    add_tokens! model SEQ_ID list(bos_token_of(model))
    add_tokens! model SEQ_ID tokens
    evaluate! model SEQ_ID
    $prefix ""
    print! PROMPT
    repeat 20:
      get_logits! $_seq_id $_position $logits model
      logits(1) $best_token $_best_logit
      detokenize model best_token $piece &prefix
      print! piece
      add_token_and_evaluate! model SEQ_ID best_token
      next!
    println!

  Output:

    Once upon a time, there was a little girl named Alice. Alice lived in a
    small village with her parents and her

<namespace llama>
<namespace llama_types>

<using std>
<using llama>

$llama::TOKENS .
$llama::LOGITS .
$llama::PROGRESS .
$llama::ERROR .

$llama::fd_of ()
$llama::pieces_of ()
$llama::is_an_end_token_of ()
$llama::bos_token_of ()
$llama::eos_token_of ()
$llama::eot_token_of ()
$llama::sep_token_of ()
$llama::nl_token_of ()
$llama::pad_token_of ()

$llama_types::model std_types::object

$llama_types::model.llama::fd_of undefined
$llama_types::model.pieces_of undefined
$llama_types::model.llama::is_an_end_token_of empty_hash_set
$llama_types::model.llama::bos_token_of undefined
$llama_types::model.llama::eos_token_of undefined
$llama_types::model.llama::eot_token_of undefined
$llama_types::model.llama::sep_token_of undefined
$llama_types::model.llama::nl_token_of undefined
$llama_types::model.llama::pad_token_of undefined

$llama::detokenize:
  (
    model
    token_or_list
    prefix = ""
  )
  $rc result_count()
  $pieces pieces_of(model)
  if
    token_or_list.is_a_list:
      $text ""
      for_each token_or_list
	: (token)
	  token_to_piece token $str &prefix
	  append &text str
	  next
	:
	  if
	    rc == 1
	    -> text
	    -> text prefix
    :
      token_to_piece token_or_list $text &prefix
      if
	rc == 1
	-> text
	-> text prefix

  $token_to_piece: (token prefix_string)
    $piece pieces(token+1)
    append prefix_string &piece
    $n length_of(piece)
    $i 1
    $e 0
    loop:
      update_if i <= n+1 &e -> i-1
      if
	i > n
	-> range(piece 1 e).from_utf8 range(piece e+1 -1)
	:
	  $code piece(i).to_integer
	  cond
	    -> code < 0x80:
	      inc &i
	      next
	    -> code & 0xe0 == 0xc0:
	      plus &i 2
	      next
	    -> code & 0xf0 == 0xe0:
	      plus &i 3
	      next
	    -> code & 0xf8 == 0xf0:
	      plus &i 4
	      next
	    -> true -> "<???>" ""


$llama::list_models:
  open_tcp_client_socket! $fd "127.0.0.1" 7683
  write_to! fd "list_models@0;"
  handle_reply! $available_models fd handle_list_models
  close! fd
  -> available_models

  $handle_list_models: (_dummy reply)
    $tag reply .truncate_from. ' '
    case tag
      "models":
	$models split(behind(reply ' ' 2))
	-> models true
      "error":
	$message reply .behind. ' '
	-> error(message) true
      -> error("unexpected reply") true

$llama::open_model ()

$std_types::string/open_model: (model_name)
  open_tcp_client_socket! $fd "127.0.0.1" 7683 true
  write_to! fd "
    use_model @(model_name)@0;@
    get_pieces@0;@
    get_special_tokens@0;@
  handle_reply! $model fd handle_model_info
  if
    model.is_an_error
    -> model
    -> model(.fd_of fd)

$std_types::io/open_model: (io id model_name)
  run io open_model_request id model_name

$open_model_request: (io id model_name)
  open_tcp_client_socket! $fd "127.0.0.1" 7683 true
  write &io fd "
    use_model @(model_name)@0;@
    get_pieces@0;@
    get_end_tokens@0;@
    get_special_tokens@0;@
  start_reading_from &io fd
  set_context &io fd tuple(id llama_types::model(.fd_of fd) "")
  register_handlers &io fd
    WRITE_FAILED = write_failed
    READ = read_data
    READ_FAILED = read_failed
  -> io undefined

$handle_model_info: (model reply)
  update_if model.is_undefined &model -> llama_types::model
  $tag reply .truncate_from. ' '
  case tag
    "pieces":
      # ignore piece count
      $pieces split(behind(reply ' ' 2))
      map &pieces hex_to_string
      -> model(.pieces_of pieces) false
    "end_tokens":
      # ignore tken count
      $tokens split(behind(reply ' ' 2))
      map &tokens to_integer
      $is_an_end_token empty_hash_set
      for_each tokens
	: (token)
	  !is_an_end_token(token) true
	  next
	-> model(.is_an_end_token_of is_an_end_token) false
    "special_tokens":
      $special_tokens split(reply .behind. ' ')
      map &special_tokens: (str)
	key_value_pair
	  str .before. '='
	  to_integer(str .behind. '=')
      for_each special_tokens
	: (special_token)
	  special_token $name $token
	  if
	    token >= 0:
	      case name
		"bos":
		  !model.llama::bos_token_of token
		  next
		"eos":
		  !model.llama::eos_token_of token
		  next
		"eot":
		  !model.llama::eot_token_of token
		  next
		"sep":
		  !model.llama::sep_token_of token
		  next
		"nl":
		  !model.llama::nl_token_of token
		  next
		"pad":
		  !model.llama::pad_token_of token
		  next
		next
	    next
	-> model true
    "error":
      $message reply .behind. ' '
      -> error(message) true
    -> error("unexpected reply (tag: @(tag))") true

$write_failed: (io fd err context)
  context $id
  set_context &io fd undefined
  -> io tuple(JOB_FAILED id err)

$read_data: (io fd data context)
  context $id $model $reply
  loop:
    search $pos $_len '@nul;' data
    if
      pos.is_defined:
	append &reply range(data 1 pos-1)
	range &data pos+1 -1
	handle_model_info &model $done reply
	if
	  done:
	    if
	      model.is_an_error
	      -> io tuple(JOB_FAILED id model)
	      :
		deregister_all_handlers &io fd
		deregister_all_handlers &io id
		set_context &io fd tuple(model "")
		register_handlers &io fd
		  WRITE_FAILED = request_failed
		  READ = read_reply
		  READ_FAILED = read_reply_failed
		-> io tuple(JOB_COMPLETED id model)
	  :
	    !reply ""
	    next
      :
	append &reply data
	set_context &io fd tuple(id model reply)
	-> io undefined

$read_failed: (io fd err context)
  context $id
  set_context &io fd undefined
  -> io tuple(JOB_FAILED id err)

$request_failed: (io fd err)
  set_context &io fd undefined
  -> io tuple(ERROR fd err)

$read_reply: (io fd data context)
  context $model $reply
  $events empty_list
  loop:
    search $pos $_len '@nul;' data
    if
      pos.is_defined:
	append &reply range(data 1 pos-1)
	range &data pos+1 -1
	push &events create_reply_event(reply model)
	!reply ""
	next
      :
	append &reply data
	set_context &io fd tuple(model reply)
	-> io events

$read_reply_failed: (io fd err)
  debug::dump `err `fd
  -> io undefined

$create_reply_event: (reply model)
  $tag reply .truncate_from. ' '
  case tag
    "tokens":
      $tokens map(split(behind(reply ' ' 2)) to_integer)
      -> tuple(TOKENS fd_of(model) tokens model)
    "logits":
      $sequence_id between(reply ' ' ' ').to_integer
      $_position between(reply ' ' ' ' 2).to_integer
      $_count between(reply ' ' ' ' 3).to_integer
      $logits behind(reply ' ' 4)
      split &logits ' '
      map &logits: (str)
	key_value_pair
	  to_integer(str .before. '=')
	  to_number(str .behind. '=')
      -> tuple(LOGITS fd_of(model) tuple(sequence_id logits) model)
    "progress":
      $sequence_id between(reply ' ' ' ').to_integer
      $remaining_tokens behind(reply ' ' 2).to_integer
      -> tuple(PROGRESS fd_of(model) tuple(sequence_id remaining_tokens) model)
    "error":
      $message reply .behind. ' '
      -> tuple(ERROR fd_of(model) message model)
    -> tuple(ERROR fd_of(model) "invalid_reply" model)

$llama_types::model/close: (model)
  close! fd_of(model)

$hex_to_string: (hex_digits)
 $i 1
 $n length_of(hex_digits)
 $str ""
 loop:
   if
     i > n
     -> str
     :
       push &str
	 character
	   (hex_to_value(hex_digits(i))<<4)+hex_to_value(hex_digits(i+1))
       plus &i 2
       next

$hex_to_value: (hex_digit)
  if
    hex_digit >= 'a'
    -> hex_digit-'a'+10
    -> hex_digit-'0'

$llama::tokenize ()

$llama_types::model/tokenize: (model prompt)
  $fd fd_of(model)
  $n 1
  if
    prompt.is_a_list:
      !n length_of(prompt)
      write_to! fd map_reduce(prompt: (text) -> "tokenize @(text.to_utf8)@0;")
      handle_reply! fd handle_tokens n
    :
      write_to! fd "
	tokenize @(prompt.to_utf8)@0;@
      handle_reply! fd handle_tokens n

$std_types::io/tokenize: (io model prompt)
  $fd fd_of(model)
  if
    prompt.is_a_list:
      write io fd map_reduce(prompt: (text) -> "tokenize @(text.to_utf8)@0;")
    :
      write io fd "
	tokenize @(prompt.to_utf8)@0;@

$handle_tokens: (prompt_tokens reply n)
  $tag reply .truncate_from. ' '
  case tag
    "tokens":
      # ignore token count
      $tokens map(split(behind(reply ' ' 2)) to_integer)
      if
	n > 0:
	  if
	    prompt_tokens.is_undefined
	    -> list(tokens) n == 1
	    :
	      push &prompt_tokens tokens
	      -> prompt_tokens length_of(prompt_tokens) == n
	-> tokens true
    "error":
      $message reply .behind. ' '
      -> error(message) true
    -> error("unexpected reply") true

$llama::create_sequence ()

$llama_types::model/create_sequence: (model sequence_id)
  $fd fd_of(model)
  write_to! fd "
    create_sequence @(sequence_id)@0;@

$std_types::io/create_sequence: (io model sequence_id)
  $fd fd_of(model)
  write io fd "
    create_sequence @(sequence_id)@0;@

$llama::copy_sequence ()

$llama_types::model/llama::copy_sequence:
  (
    model
    sequence_id
    template_id
    token_count
  )
  $fd fd_of(model)
  write_to! fd "
    copy_sequence @(sequence_id) @(template_id) @(token_count)@0;@

$std_types::io/llama::copy_sequence:
  (
    io
    model
    sequence_id
    template_id
    token_count
  )
  $fd fd_of(model)
  write io fd "
    copy_sequence @(sequence_id) @(template_id) @(token_count)@0;@

$llama::delete_sequence ()

$llama_types::model/llama::delete_sequence: (model sequence_id)
  $fd fd_of(model)
  write_to! fd "
    delete_sequence @(sequence_id)@0;@

$std_types::io/llama::delete_sequence: (io model sequence_id)
  $fd fd_of(model)
  write io fd "
    delete_sequence @(sequence_id)@0;@

$llama::truncate_sequence ()

$llama_types::model/llama::truncate_sequence: (model sequence_id token_count)
  $fd fd_of(model)
  write_to! fd "
    truncate_sequence @(sequence_id) @(token_count)@0;@

$std_types::io/llama::truncate_sequence: (io model sequence_id token_count)
  $fd fd_of(model)
  write io fd "
    truncate_sequence @(sequence_id) @(token_count)@0;@

$llama::add_tokens ()

$llama_types::model/llama::add_tokens: (model sequence_id tokens)
  $fd fd_of(model)
  $token_string map_reduce(tokens to_string concatenate(" "))
  write_to! fd "
    add_tokens @(sequence_id) @(length_of(tokens)) @(token_string)@0;@

$std_types::io/llama::add_tokens: (io model sequence_id tokens)
  $fd fd_of(model)
  $token_string map_reduce(tokens to_string concatenate(" "))
  write io fd "
    add_tokens @(sequence_id) @(length_of(tokens)) @(token_string)@0;@

$llama::add_token_and_evaluate ()

$llama_types::model/llama::add_token_and_evaluate: (model sequence_id token)
  $fd fd_of(model)
  write_to! fd "
    add_tokens @(sequence_id) 1 @(token)@0;@
    evaluate @(sequence_id)@0;@

$std_types::io/llama::add_token_and_evaluate: (io model sequence_id token)
  $fd fd_of(model)
  write io fd "
    add_tokens @(sequence_id) 1 @(token)@0;@
    evaluate @(sequence_id)@0;@

$llama::delete_tokens ()

$llama_types::model/llama::delete_tokens:
  (
    model
    sequence_id
    position
    token_count
  )
  $fd fd_of(model)
  write_to! fd "
    delete_tokens @(sequence_id) @(position) @(token_count)@0;@

$std_types::io/llama::delete_tokens:
  (
    io
    model
    sequence_id
    position
    token_count
  )
  $fd fd_of(model)
  write io fd "
    delete_tokens @(sequence_id) @(position) @(token_count)@0;@

$llama::insert_tokens ()

$llama_types::model/llama::insert_tokens:
  (
    model
    sequence_id
    position
    tokens
  )
  $fd fd_of(model)
  $token_string map_reduce(tokens to_string concatenate(" "))
  write_to! fd "
    insert_tokens @(sequence_id) @(position) @(length_of(tokens)) @
    @(token_string)@0;@

$std_types::io/llama::insert_tokens:
  (
    io
    model
    sequence_id
    position
    tokens
  )
  $fd fd_of(model)
  $token_string map_reduce(tokens to_string concatenate(" "))
  write io fd "
    insert_tokens @(sequence_id) @(position) @(length_of(tokens)) @
    @(token_string)@0;@

$llama::evaluate ()

$llama_types::model/llama::evaluate:
  (
    model
    sequence_id
  )
  $fd fd_of(model)
  write_to! fd "
    evaluate @(sequence_id)@0;@

$std_types::io/llama::evaluate:
  (
    io
    model
    sequence_id
  )
  $fd fd_of(model)
  write io fd "
    evaluate @(sequence_id)@0;@

$llama::get_logits: (model)
  $fd fd_of(model)
  handle_reply! $result fd handle_logits
  result

  $handle_logits: (_dummy reply)
    $tag reply .truncate_from. ' '
    case tag
      "logits":
	$sequence_id between(reply ' ' ' ').to_integer
	$position between(reply ' ' ' ' 2).to_integer
	$logits behind(reply ' ' 4)
	split &logits ' '
	map &logits: (str)
	  key_value_pair
	    to_integer(str .before. '=')
	    to_number(str .behind. '=')
	-> tuple(sequence_id position logits) true
      "error":
	$message reply .behind. ' '
	-> error(message) true
      -> error("unexpected reply") true

$handle_reply: (fd handler handler_data)
  $reply ""
  $result undefined
  loop:
    read! $data fd 0x40000
    if
      data.is_empty
      error("CONNECTION CLOSED")
      :
	loop
	  :
	    search $pos $_len '@nul;' data
	    if
	      pos.is_defined:
		append &reply range(data 1 pos-1)
		range &data pos+1 -1
		if !result $done
		  handler_data.is_defined:
		    handler result reply handler_data
		  :
		    handler result reply
		if
		  done
		  -> result
		  :
		    !reply ""
		    next!
	      :
		append &reply data
		break!
	  next
