#
  Copyright (C) 2023 by
  Dipl.-Ing. Michael Niederle

  This program is free software; you can redistribute it and/or modify
  it under the terms of the GNU Library General Public License, version 2, or
  (at your option) under the terms of the GNU Lesser General Public License,
  version 3.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
  GNU Lesser (Library) General Public License for more details.

  For details of the GNU General Public License see the accompanying
  files LGPLv2.txt and LGLPv3.txt or
  http://www.gnu.org/licenses/lgpl-2.0.html
  http://www.gnu.org/licenses/lgpl-3.0.html
  or print to the
  Free Software Foundation, Inc.,
  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.

<namespace ai>
<namespace ai_types>

#
  Topic: Deprecated AI

  This API is deprecated.

  The "funky_server" needed as a backend is no longer maintained.

  Use ai/new_llama instead.

# Modes

$ai::EXACT . # evaluate the result using the full context size (slow)
$ai::SMART .

$DEFAULT_PORT_NO 8080

# Attributes

$ai::pieces_of ()
$ai::piece_table_of ()
$ai::maximum_piece_length_of ()
$ai::begin_of_stream_token_of ()
$ai::end_of_stream_token_of ()
$ai::classsification_token_of ()
$ai::separator_token_of ()
$ai::newline_token_of ()
$ai::space_token_of ()
$ai::end_of_text_token_of ()
$ai::prefix_token_of ()
$ai::suffix_token_of ()
$ai::middle_token_of ()
$ai::question_token_of ()
$ai::answer_token_of ()
$ai::context_size_of ()
$ai::effective_context_size_of ()
  #
    the context size to effectively use with the model

    must be less or equal to <ai::context_size_of> (or <std::undefined>)

$std::address_of ()
$std::port_no_of ()

$uuid_of ()

# Methods

$ai::cached_prefix ()
$ai::evaluate ()
$ai::tokenize ()
$ai::detokenize ()
$ai::generate ()
$ai::deregister ()

# Options

$ai::SERVER_NAME .
$ai::CONTEXT_SIZE .
$ai::MODEL_PATH .
$ai::MINIMUM_CONFIDENCE .
$ai::MAXIMUM_LENGTH .
$ai::STOP .
$ai::BACKTRACK .
$ai::LOG_LEVEL .
$ai::VERBOSE .
$ai::USE_COLOURS .

# Prototype Objects

$ai_types::connection std_types::resource

$is_a_connection ()

$std_types::object.is_a_connection false
$ai_types::connection.is_a_connection true

$ai_types::connection.address_of undefined
$ai_types::connection.port_no_of undefined
$ai_types::connection.uuid_of undefined
$ai_types::connection.name_of "connection"

$DEREGISTER .

$ai_types::connection/finalize:
  (
    self
    io
  )
  $address address_of(self)
  $port_no port_no_of(self)
  $uuid uuid_of(self)
  $id tuple(DEREGISTER uuid)
  log &io 3 "deregistering from AI server at address @(address):@(port_no)"
  http_request! &io $_events id address port_no deregister_request(uuid)
  register_handlers io id
    JOB_COMPLETED = deregister_completed
    JOB_FAILED = deregister_failed

$deregister_completed: (io id _data)
  log &io 3 "successfully deregistered from AI server"
  deregister_all_handlers &io id
  -> io undefined

$deregister_failed: (io id _data)
  log &io "failed to deregister from AI server"
  deregister_all_handlers &io id
  -> io undefined

$ai_types::model std_types::object

$ai_types::model.name_of undefined
$ai_types::model.address_of undefined
$ai_types::model.port_no_of undefined
$ai_types::model.uuid_of undefined
$ai_types::model.ai::pieces_of undefined
$ai_types::model.ai::piece_table_of undefined
$ai_types::model.ai::maximum_piece_length_of undefined
$ai_types::model.ai::begin_of_stream_token_of undefined
$ai_types::model.ai::end_of_stream_token_of undefined
$ai_types::model.ai::classsification_token_of undefined
$ai_types::model.ai::separator_token_of undefined
$ai_types::model.ai::newline_token_of undefined
$ai_types::model.ai::space_token_of undefined
$ai_types::model.ai::end_of_text_token_of undefined
$ai_types::model.ai::prefix_token_of undefined
$ai_types::model.ai::suffix_token_of undefined
$ai_types::model.ai::middle_token_of undefined
$ai_types::model.ai::question_token_of undefined
$ai_types::model.ai::answer_token_of undefined
$ai_types::model.ai::context_size_of undefined
$ai_types::model.ai::effective_context_size_of undefined

# Implementation

$ai::list_ai_models:
  (
     options*
  )
  get_options options
    ADDRESS = "127.0.0.1" $address
    PORT_NO = DEFAULT_PORT_NO $port_no
    UUID = undefined $uuid
  $json
    to_json
      insert_order_table
	"uuid" = uuid
  to_utf8 &json
  open_tcp_client_socket! $fd address port_no
  write_to! fd "
    POST /list_models HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)
  load! $result fd
  on result.is_an_error result
  behind &result "@cr;@nl;@cr;@nl;"
  from_utf8 &result
  $info result.from_json
  -> info("models")

$ai::load_ai_model ()

$std_types::string/ai::load_ai_model:
  (
    name
    options*
  )
  initialize_model $model name options
  get_tokens! model
    !model.ai::pieces_of
    !model.ai::begin_of_stream_token_of
    !model.ai::end_of_stream_token_of
    !model.ai::classsification_token_of
    !model.ai::separator_token_of
    !model.ai::newline_token_of
    !model.ai::end_of_text_token_of
    !model.ai::prefix_token_of
    !model.ai::suffix_token_of
    !model.ai::middle_token_of
    !model.ai::context_size_of
  if
    ai::pieces_of(model).is_an_error
    -> ai::pieces_of(model)
    :
      build_piece_table &model

      #
	The default special tokens are only meant to be used with LLama models
	So let's replace them for other models (especially Falcon).

      !model.ai::newline_token_of ai::piece_table_of(model)("@nl;")
      !model.ai::space_token_of ai::piece_table_of(model)(" ")

      for_each ai::pieces_of(model)
	: (token piece)
	  if
	    &&
	      piece.is_a_string
	      length_of(piece) > 4
	      ||
		piece .has_prefix. "<|" && piece .has_suffix. "|>"
		piece .has_prefix. ">>" && piece .has_suffix. "<<"
	    :
	      $special range(piece 3 -3)
	      case special
		"endoftext":
		  !model.ai::end_of_text_token_of token-1
		  next
		"QUESTION":
		  !model.ai::question_token_of token-1
		  next
		"ANSWER":
		  !model.ai::answer_token_of token-1
		  next
		"PREFIX":
		  !model.ai::prefix_token_of token-1
		  next
		"SUFFIX":
		  !model.ai::suffix_token_of token-1
		  next
		"MIDDLE":
		  !model.ai::middle_token_of token-1
		  next
		next
	    next
	-> model

$model_of ()

$std_types::io.model_of empty_hash_table

$std_types::io/ai::load_ai_model:
  (
    io
    id
    name
    options*
  )
  update_if
    &&
      length_of(options) == 1
      ||
	options(1).is_a_list
	options(1).is_a_connection
    &options -> options(1)
  initialize_model $model name options
  tuple &id 1
  run &io http_request id
    address_of(model) port_no_of(model) get_tokens_request(model)
  !io.model_of(id) model
  register_handlers io id
    JOB_COMPLETED = load_ai_model_completed
    JOB_FAILED = load_ai_model_failed

$load_ai_model_completed: (io id data)
  $model model_of(io)(id)
  !io.model_of(id) undefined
  deregister_all_handlers &io id
  id !id
  from_utf8 &data
  from_json &data
  if
    data.is_an_error || data.is_undefined
    -> io tuple(JOB_FAILED id error(IO_ERROR "INVALID JSON"))
    :
      !model
	.ai::pieces_of data("tokens")
	.ai::begin_of_stream_token_of data("begin_of_stream")
	.ai::end_of_stream_token_of data("end_of_stream")
	.ai::classsification_token_of data("classification")
	.ai::separator_token_of data("separator")
	.ai::newline_token_of data("newline")
	.ai::end_of_text_token_of data("end_of_text")
	.ai::prefix_token_of data("prefix")
	.ai::suffix_token_of data("suffix")
	.ai::middle_token_of data("middle")
	.ai::context_size_of data("context_size")
      build_piece_table &model
      !model.ai::space_token_of ai::piece_table_of(model)(" ")
      -> io tuple(JOB_COMPLETED id model)

$load_ai_model_failed: (io id err)
  !io.model_of(id) undefined
  deregister_all_handlers &io id
  id !id
  -> io tuple(JOB_FAILED id err)

$initialize_model: (name options)
  if
    options.is_a_connection
    ->
      ai_types::model
	.name_of name
	.address_of address_of(options)
	.port_no_of port_no_of(options)
	.uuid_of uuid_of(options)
	.ai::effective_context_size_of undefined
    :
      get_options options
	ADDRESS = "127.0.0.1" $address
	PORT_NO = DEFAULT_PORT_NO $port_no
	UUID = undefined $uuid
	ai::CONTEXT_SIZE = undefined $context_size
      ->
	ai_types::model
	  .name_of name
	  .address_of address
	  .port_no_of port_no
	  .uuid_of uuid
	  .ai::effective_context_size_of context_size

$ai_types::model/ai::generate:
  (
    model
    options*
    prompt
  )
  get_options options
    ai::MINIMUM_CONFIDENCE = 15 $minimum_confidence
    ai::MAXIMUM_LENGTH = 1024 $maximum_length
    ai::STOP = undefined $stop_text
    ai::BACKTRACK = 0 $backtrack
    ai::LOG_LEVEL = 0 $log_level
    ai::VERBOSE = false $be_verbose
    ai::USE_COLOURS = false $use_colours
  $do_log log_level > 0
  update_if use_colours &be_verbose -> true
  on use_colours: print! ansi_text_colour(BLACK)
  ai::tokenize! $prompt_tokens model prompt
  generate_text! $result_text $_max_no2 prompt_tokens "" 0 empty_list
  on use_colours: print! ansi_reset_colour()
  on be_verbose: println!
  if
    result_count() == 1
    -> result_text
    pass

  $generate_text: (tokens text no prefix)
    if
      stop_text.is_defined && text .contains. stop_text
      -> text no
      :
	current_time! $t1
	ai::evaluate! model tokens $next_token $confidence_values
	if
	  next_token == ai::end_of_stream_token_of(model):
	    on do_log:
	      eprintln! "<end of stream>"
	    -> text no
	  :
	    $max_no no
	    loop:
	      confidence_values(1) !next_token $next_confidence
	      ai::detokenize model next_token prefix $next_piece $suffix
	      on do_log:
		current_time! $t2
		$dt 1000*(t2-t1) # in ms
		if
		  log_level <= 1:
		    $token_string
		      "@quot;@(ai::escaped_piece(next_piece))@quot;:"
		    eprintln!
		      format
			"[#%4] %l18 %3.3"
			no token_string next_confidence
		  :
		    if
		      log_level >= 3:
			eprintln! format("-[#%4|%3.1 ms]---------------" no dt)
		      :
			eprintln! format("-[#%4]------------------------" no dt)
		    for_each confidence_values
		      : (confidence_value)
			confidence_value $token $confidence
			ai::detokenize model token prefix $piece
			replace_all &piece '@ht;' = "<<<ht>>>"
			$token_string "@quot;@(ai::escaped_piece(piece))@quot;:"
			eprintln!
			  format
			    "%5 %l18 %3.3" token token_string confidence
			next!
		      pass
	      if
		next_confidence >= minimum_confidence || no < 5:
		  update_if no == 0 &next_piece: trim_left next_piece
		  print_piece! next_piece next_confidence
		  if
		    no >= maximum_length
		    -> append(text next_piece) no
		    :
		      generate_text! $response $highest_no
			push(tokens next_token)
			append(text next_piece)
			no+1
			suffix
		      max &max_no highest_no
		      if
			response.is_empty:
			  if
			    next_piece == "@nl;"
			    -> push(text '@nl;') max_no # stop backtracking
			    :
			      if
				||
				  length_of(confidence_values) < 2
				  max_no > no+backtrack
				-> "" max_no
				:
				  on use_colours: print! ansi_reset_colour()
				  on be_verbose: print!
				    dup("@bs; @bs;" length_of(next_piece))
				  on use_colours: print! ansi_text_colour(BLACK)
				  range &confidence_values 2 -1
				  next!
			-> response max_no
		-> "" max_no

  $print_piece: (piece confidence)
    on use_colours:
      $confidence_delta confidence-minimum_confidence
      $colour
	cond
	  -> confidence_delta < 0 ->
	    colour_mixture
	      BLACK = -confidence_delta
	      RED = minimum_confidence+confidence_delta
	  -> confidence_delta > 10 ->
	    colour_mixture
	      GREEN = 10
	      WHITE = confidence_delta-10
	  -> true ->
	    colour_mixture
	      RED = 10-confidence_delta
	      GREEN = confidence_delta
      print! ansi_background_colour(colour)
    on be_verbose: print! piece

$get_tokens: (model)
  $request get_tokens_request(model)
  open_tcp_client_socket! $fd address_of(model) port_no_of(model)
  print_to! fd request
  load! $result fd
  $error_code between(result ' ' ' ').to_integer
  if
    error_code >= 300:
      error result .behind. "@cr;@nl;@cr;@nl;"
    :
      behind &result "@cr;@nl;@cr;@nl;"
      from_utf8 &result
      $info result.from_json
      ->
	info("tokens")
	info("begin_of_stream")
	info("end_of_stream")
	info("classification")
	info("separator")
	info("newline")
	info("end_of_text")
	info("prefix")
	info("suffix")
	info("middle")
	info("context_size")

$get_tokens_request: (model)
  $options
    insert_order_table
      "model" = name_of(model)
      "uuid" = uuid_of(model)
  update_if ai::effective_context_size_of(model).is_defined &options
    -> options("context_size" ai::effective_context_size_of(model))
  $json options.to_json
  to_utf8 &json
  -> "
    POST /get_tokens HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)@

$ai_types::model/ai::tokenize:
  (
    model
    text
  )
  $request tokenize_request(model text)
  open_tcp_client_socket! $fd address_of(model) port_no_of(model)
  write_to! fd request
  load! $result fd
  behind &result "@cr;@nl;@cr;@nl;"
  from_utf8 &result
  $info result.from_json
  -> info("tokens")

$std_types::io/ai::tokenize:
  (
    io
    id
    model
    text
  )
  tuple &id 1
  run &io http_request id
    address_of(model) port_no_of(model) tokenize_request(model text)
  register_handlers io id
    JOB_COMPLETED = tokenize_completed
    JOB_FAILED = tokenize_failed

$tokenize_completed: (io id data)
  deregister_all_handlers &io id
  id !id
  from_utf8 &data
  from_json &data
  if
    data.is_an_error || data.is_undefined
    -> io tuple(JOB_FAILED id error(IO_ERROR "INVALID JSON"))
    -> io tuple(JOB_COMPLETED id data("tokens"))

$tokenize_failed: (io id err)
  deregister_all_handlers &io id
  id !id
  -> io tuple(JOB_FAILED id err)

$tokenize_request: (model text)
  $CONTENT
    if
      text.is_a_string
      -> "content"
      -> "lines"
  $json
    to_json
      insert_order_table
	"model" = name_of(model)
	"uuid" = uuid_of(model)
	CONTENT = text
  to_utf8 &json
  -> "
    POST /tokenize HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)@

$ai_types::model/ai::evaluate:
  (
    model
    prompt
    mode = ai::SMART
    start = 0
    additional = 0
  )
  $do_return_confidence_values result_count() == 2
  $use_tokens not(prompt.is_a_string)
  $request evaluate_request(model prompt mode start additional)
  open_tcp_client_socket! $fd address_of(model) port_no_of(model)
  write_to! fd request
  load! $result fd
  behind &result "@cr;@nl;@cr;@nl;"
  from_utf8 &result
  $info result.from_json
  if
    do_return_confidence_values:
      $logits info("logits")
      map &logits: (logit) tuple logit(1) logit(2)
      if
	use_tokens
	-> info("token") logits
	-> info("content") logits
    :
      if
	use_tokens
	-> info("token")
	-> info("content")

$std_types::io/ai::evaluate:
  (
    io
    id
    model
    prompt
    mode = ai::SMART
    start = 0
    additional = 0
  )
  tuple &id 1
  run &io http_request id
    address_of(model) port_no_of(model)
    evaluate_request(model prompt mode start additional)
  register_handlers io id
    JOB_COMPLETED = evaluate_completed
    JOB_FAILED = evaluate_failed

$evaluate_completed: (io id data)
  deregister_all_handlers &io id
  id !id
  from_utf8 &data
  from_json &data
  if
    data.is_an_error || data.is_undefined
    -> io tuple(JOB_FAILED id error(IO_ERROR "INVALID JSON"))
    -> io tuple(JOB_COMPLETED id data("logits"))

$evaluate_failed: (io id err)
  deregister_all_handlers &io id
  id !id
  -> io tuple(JOB_FAILED id err)

$evaluate_request: (model prompt mode start additional)
  $use_tokens not(prompt.is_a_string)
  $PROMPT
    if
      use_tokens
      -> "tokens"
      -> "prompt"
  case &mode
    ai::EXACT -> "exact"
    ai::SMART -> "smart"
    -> "exact"
  $json
    to_json
      insert_order_table
	PROMPT = prompt
	"model" = name_of(model)
	"uuid" = uuid_of(model)
	"mode" = mode
	"start" = start
	"additional" = additional
	"logits" = true # also disables server side token selection
	"brief" = true
	"n_probs" = 10
	"n_predict" = 1
  to_utf8 &json
  -> "
    POST /completion HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)

$std_types::io/ai::cached_prefix:
  (
    io
    id
    model
    prompt
    mode = ai::SMART
    start = 0
    additional = 0
  )
  tuple &id 1
  run &io http_request id
    address_of(model) port_no_of(model)
    cached_prefix_request(model prompt mode start additional)
  register_handlers io id
    JOB_COMPLETED = cached_prefix_completed
    JOB_FAILED = cached_prefix_failed

$cached_prefix_completed: (io id data)
  deregister_all_handlers &io id
  id !id
  from_utf8 &data
  from_json &data
  if
    data.is_an_error || data.is_undefined
    -> io tuple(JOB_FAILED id error(IO_ERROR "INVALID JSON"))
    ->
      io
      tuple
	JOB_COMPLETED
	id
	data("cached_prefix_length").to_integer
	data("additional_length").to_integer

$cached_prefix_failed: (io id err)
  deregister_all_handlers &io id
  id !id
  -> io tuple(JOB_FAILED id err)

$cached_prefix_request: (model prompt mode start additional)
  $use_tokens not(prompt.is_a_string)
  $PROMPT
    if
      use_tokens
      -> "tokens"
      -> "prompt"
  case &mode
    ai::EXACT -> "exact"
    ai::SMART -> "smart"
    -> "exact"
  $json
    to_json
      insert_order_table
	PROMPT = prompt
	"model" = name_of(model)
	"uuid" = uuid_of(model)
	"mode" = mode
	"start" = start
	"additional" = additional
  to_utf8 &json
  -> "
    POST /cached_prefix HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)

$ai_types::model/ai::detokenize:
  (
    model
    token_or_list
    prefix = empty_list
  )
  $rc result_count()
  $pieces ai::pieces_of(model)
  if
    token_or_list.is_a_list:
      $text ""
      for_each token_or_list
	: (token)
	  token_to_piece token $str &prefix
	  append &text str
	  next
	:
	  if
	    rc == 1
	    -> text
	    -> text prefix
    :
      token_to_piece token_or_list $str &prefix
      if
	rc == 1
	-> str
	-> str prefix

  $token_to_piece: (token prefix_octets)
    $piece pieces(token+1)
    if
      piece.is_a_list:
	append prefix_octets &piece
	$n length_of(piece)
	$i 1
	$e undefined
	loop:
	  update_if i-1 <= n &e -> i-1
	  if
	    i > n:
	      $octets ""
	      for_each range(piece 1 e)
		: (code)
		  push &octets character(code)
		  next
		-> from_utf8(octets) range(piece e+1 -1)
	    :
	      $code piece(i)
	      cond
		-> code < 0x80:
		  inc &i
		  next
		-> code & 0xe0 == 0xc0:
		  plus &i 2
		  next
		-> code & 0xf0 == 0xe0:
		  plus &i 3
		  next
		-> code & 0xf8 == 0xf0:
		  plus &i 4
		  next
		-> true -> "<???>" empty_list
      :
	update_if any_of(piece: (chr) -> chr == '@0x142;') &piece -> "<???>"
	  # 'ł' cannot be printed - why?
	-> piece empty_list

$ai::connect_to_ai_server:
  (
    io
    id
    options*
  )
  update_if length_of(options) == 1 && options(1).is_a_list
    &options -> options(1)
  get_options options
    ADDRESS = "127.0.0.1" $address
    PORT_NO = DEFAULT_PORT_NO $port_no
    UUID = undefined $uuid
  tuple &id 1
  run &io http_request id address port_no register_request(uuid)
  !io.connection_of(id)
    ai_types::connection
      .address_of address
      .port_no_of port_no
      .uuid_of uuid
  register_handlers io id
    JOB_COMPLETED = connect_completed
    JOB_FAILED = connect_failed

$connection_of ()

$std_types::io.connection_of empty_hash_table

$connect_completed: (io id _data)
  $connection connection_of(io)(id)
  !io.connection_of(id) undefined
  deregister_all_handlers &io id
  id !id
  -> io tuple(JOB_COMPLETED id connection)

$connect_failed: (io id err)
  !io.connection_of(id) undefined
  deregister_all_handlers &io id
  id !id
  -> io tuple(JOB_FAILED id err)

$register_request: (uuid)
  $json
    to_json
      insert_order_table
	"uuid" = uuid
  to_utf8 &json
  -> "
    POST /register HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)@

$std_types::string/ai::deregister:
  (
    uuid
    address = "127.0.0.1"
    port_no = DEFAULT_PORT_NO
  )
  sync_deregister! address port_no uuid

$ai_types::model/ai::deregister:
  (
    model
  )
  ai::deregister! uuid_of(model) "@(address_of(model)):@(port_no_of(model))"

$std_types::io/ai::deregister:
  (
    io
    id
    uuid_or_model
    address = "127.0.0.1"
    port_no = DEFAULT_PORT_NO
  )
  if
    uuid_or_model.is_a_string:
      async_deregister io id address port_no uuid_or_model
    :
      async_deregister io id
	address_of(uuid_or_model)
	port_no_of(uuid_or_model)
	uuid_of(uuid_or_model)

$sync_deregister: (address port_no uuid)
  open_tcp_client_socket! $fd address port_no
  write_to! fd deregister_request(uuid)
  load! $_result fd
  pass

$async_deregister: (io id address port_no uuid)
  run io http_request id address port_no deregister_request(uuid)

$deregister_request: (uuid)
  $json
    to_json
      insert_order_table
	"uuid" = uuid
  to_utf8 &json
  -> "
    POST /deregister HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)@

$build_piece_table: (model)
  $pieces ai::pieces_of(model)
  $piece_table empty_hash_table
  for_each pieces
    : (idx piece)
      if
	piece.is_a_string:
	  if
	    piece_table(piece).is_defined
	    next
	    : # do not redefine a piece table entry
	      !piece_table(piece) idx-1
	      next
	next # ignore incomplete sequences
    ->
      model
	.ai::piece_table_of piece_table
	.ai::maximum_piece_length_of map_reduce(pieces length_of max)

$ai::escaped_token: (model token)
  -> ai::escaped_piece(ai::detokenize(model token))

$ai::escaped_piece: (piece)
  $buf ""
  $s 1
  $n length_of(piece)
  $i 0
  loop:
    inc &i
    if
      i > n
      -> append(buf range(piece s n))
      :
	$chr piece(i)
	case chr
	  '@nl;':
	    append &buf range(piece s i-1)
	    append &buf "\n"
	    !s i+1
	    next
	  '@quot;', '\':
	    append &buf range(piece s i-1)
	    push &buf '\'
	    push &buf chr
	    !s i+1
	    next
	  next

$ai::start_ai_server: (io id options*)
  flatten &options
  run io start_ai_server_action id options

$start_ai_server_action: (io id options)
  get_options options
    ai::SERVER_NAME = "funky_server" $cmd
    UUID = undefined $uuid
    LOG_FILE = undefined $log_file
    ADDRESS = undefined $address
    PORT_NO = undefined $port_no
    ai::CONTEXT_SIZE = undefined $context_size
    ai::MODEL_PATH = undefined $model_path
  $server_options empty_list
  update_if uuid.is_defined &server_options:
    append server_options list("--uuid" uuid)
  update_if address.is_defined &server_options:
    append server_options list("--host" address)
  update_if port_no.is_defined &server_options:
    append server_options list("--port" port_no.to_string)
  update_if context_size.is_defined &server_options:
    append server_options list("--context-size" context_size.to_string)
  update_if model_path.is_defined &server_options:
    append server_options list("--model-path" model_path)
  open! $fd_null "/dev/null" "r"
  $fd_log fd_null
  if
    log_file.is_defined:
      open! !fd_log log_file "w+"
      start_server!
    start_server

  $start_server:
    create_process! $pid cmd server_options undefined fd_null fd_log fd_log
    on fd_log != fd_null: close! fd_log
    close! fd_null
    if
      pid.is_an_error
      -> io tuple(JOB_FAILED id pid)
      :
	tuple &id 1
	run_at &io 2 http_request id address port_no register_request(uuid)
	!io.connection_of(id)
	  ai_types::connection
	    .address_of address
	    .port_no_of port_no
	    .uuid_of uuid
	register_handlers &io id
	  JOB_COMPLETED = connect_completed
	  JOB_FAILED = connect_failed
	-> io undefined

#
  $list_models_completed: (io id data)
    deregister_all_handlers &io id
    id !id
    behind &data "@cr;@nl;@cr;@nl;"
    from_utf8 &data
    $info data.from_json
    -> io tuple(JOB_COMPLETED id info)

  $list_models_failed: (io id err)
    debug::dump 50 `err
    deregister_all_handlers &io id
    id !id
    -> io tuple(JOB_FAILED id err)

  $list_models_request: (uuid)
    $json
      to_json
	insert_order_table
	  "uuid" = uuid
    to_utf8 &json
    -> "
      POST /list_models HTTP/1.1@cr;
      Content-Type: application/json@cr;
      Content-Length: @(length_of(json))@cr;
      Connection: close@cr;
      @cr;
      @(json)@
