# AI

  Funky currently supports large language models based on the llama.cpp project.

  The "fllama" tool is a server program that must be started first before a
  client can send queries to the server.

  A short glossary:

  * An *LLM* is a Large Language Model.

  * A *modelname* is the name of an LLM in GGUF-format including the ".gguf"
    file extension.

  * A *model* is an object (<llama_types::model>) on the client side that
    represents an LLM.

  * A *token* is a non-negative integer number that represents one ore more
    (maybe partial) characters.

  * A *piece* is a raw octet string associated with a token. This string can
    contain partial UTF-8-sequences. Most models do not contain so many tokens
    to be able to encode all possible Unicode code points. So smileys might
    be represented by two tokens each describing a part of a single code point.

  * A *sequence* is a sequence of tokens stored on the server. Such a sequence
    contains prompts (supplied by the client) and answers (generated by the
    server). Sequences can be truncated, some tokens can be deleted or new
    tokens can be inserted. This might harm the quality of later evaluations
    because of rounding errors. Sequences can also be copied to share a common
    prefix string (e.g. some instructions) with each other.

  * A *sequence-id* is a client supplied non-negative integer used to identify
    a sequence.

  * *logits* are float values that describe how good a token is suited to
    continue a sequence. The higher the value the better the token represents
    the "inner state" of the model. Logit values are logarithmic, so a slightly
    higher value represents a much better match. The absolute value of logits
    are model dependent.

  * *Tokenization* is the conversion of a text string into tokens.

  * *Detokenization* is the conversion of tokens into a text stringl

  * *Evaluation* is the generation of a list of logits to choose from for the
    next token based on the tokens already in the sequence

  Example:

    <require basic/stdlib>
    <require ai/new_llama>

    <using std>
    <using llama>

    $MODEL_NAME "sauerkrautlm-una-solar-instruct.Q5_K_M.gguf"
    $PROMPT "Once upon a time"

    $SEQ_ID 1
    open_model! $model MODEL_NAME
    tokenize! $tokens model PROMPT
    create_sequence! model SEQ_ID
    add_tokens! model SEQ_ID list(bos_token_of(model))
    add_tokens! model SEQ_ID tokens
    evaluate! model SEQ_ID
    $prefix ""
    print! PROMPT
    repeat 20:
      get_logits! $_seq_id $_position $logits model
      logits(1) $best_token $_best_logit
      detokenize model best_token $piece &prefix
      print! piece
      add_token_and_evaluate! model SEQ_ID best_token
      next!
    println!

  Output:

    Once upon a time, there was a little girl named Alice. Alice lived in a
    small village with her parents and her

((defined in ai/new_llama.fky))
