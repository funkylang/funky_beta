[MANUAL]

In Funky function calls are either written on a single line with arguments
enclosed within parentheses and separated by single spaces, e.g.:

print! f(a b c) g(x y z)

or they are written in indentation style using mutliple lines:

print!
  f
    a
    b
    c
  g
    x
    y
    z

Functions that create any output or read any input need I/O-access rights.

A function that needs I/O-access rights must be called with a trailing
exclamation mark after the function name, e.g.:

println! "Hello, World!"

The expression

key = value

denotes a key-value-pair.

Collection types cannot be printed directly. You can use "dump!" to
dump the contents of a collection, e.g.:

$ages
  hash_table
    "John" = 42
    "Mary" = 25
    "Irene" = 49

dump! `ages

Output:

ages: hash_table <6>
  "Mary"
  25
  "Irene"
  49
  "John"
  42

Note: The number in angular brackets after the typename denotes the number
of (indented) lines that follow the head.

A tail call returns the result of the called function; e.g.:

$calc:
  (
    x
    y
  )
  plus x*x y*y

println! calc(2 3)

prints

13

[GUIDELINES]

All documentation remarks must have

* a single short (single line) description written in lower case

* a parameter(s) section (if appropriate)

* a result(s) section (if appropriate)

* a detailed description of how to use the symbol: when describing unique
  items of constants, describe how to use them;
  do not repeat yourself

* one or more associated topic(s) (if appropriate)

* a "See also" entry - mention only a view relevant symbols here

Do not mention unknown symbols!

Do not mention more than 10 symbols in the "See also" entry.

DO NOT MENTION MORE THAN 10 SYMBOLS IN THE "See also" ENTRY!

Do not mention internal details of functions.

Do not start each sentence in the description with "If" or "The".

DO NOT START EACH SENTENCE IN THE DESCRIPTION WITH "If" OR "The"!

Do not tell the user what a function does not do.

When referring to another symbol write it enclosed in angle brackets, e.g.
<std::clear>.

The "self"-parameter is a normal parameter like any other and should be
mentioned in the the parameter description.

Replace symbolic constants by their value.

Operations on I/O-objects are not executed immediately but queued as jobs.
To identify a job, an "id" is used.

I/O-functions and I/O-methods either need I/O-access rights or propagate
them ("if", "for_each", etc.).

An "example" section, if available, must use Funky syntax and shall not be too
long.

If there is an appropriate example in the source code then use this example
in the "example" section!

[EXAMPLES]

[symbols]

$std::CURSOR_LEFT .
$std::CURSOR_RIGHT .
$std::CURSOR_UP .
$std::CURSOR_DOWN .
$std::CURSOR_HOME .
$std::CURSOR_END .

$std::DRAW_COLOUR .

$std::BLACK '@0x000000;'
$std::WHITE '@0xffffff;'
$std::RED '@0xff0000;'
$std::GREEN '@0x00ff00;'
$std::BLUE '@0x0000ff;'
$std::CYAN '@0x00ffff;'
$std::MAGENTA '@0xff00ff;'
$std::YELLOW '@0xffff00;'

$std::set_draw_colour ()
$std::set_clear_colour ()

$std_types::screen/create_screen:
  (
    screen
    width = undefined
    height = undefined
    options*
  )
  update_if width.is_undefined &width -> width_of(screen)
  update_if height.is_undefined &height -> height_of(screen)
  get_options options
    DRAW_COLOUR = draw_colour_of(screen) $draw_colour
    CLEAR_COLOUR = clear_colour_of(screen) $clear_colour
    LINE_HEIGHT = line_height_of(screen) $line_height
  $row dup(string(' ' draw_colour clear_colour) width)
  $rows dup(list(row) height .div. line_height)
  ->
    std_types::screen
      .width_of width
      .height_of height
      .draw_colour_of draw_colour
      .clear_colour_of clear_colour
      .line_height_of line_height
      .rows_of rows

[source]

$std::RED '0xff0000;'

[documentation remark]

# std::RED (a literal)

  the colour red as used by the terminal library

  Topic: Colours

  Colour literals are used to set the draw or clear colour.

  See also: std::set_draw_colour, std::set_clear_colour

[source]

$std::CURSOR_LEFT .
  #
    Example:

      $x ...
      get_key! $key
      case key
        CURSOR_LEFT:
          dec &x
          next!
        ...

[documentation remark]

# std::CURSOR_HOME (a unique item)

  the cursor home key

  Topic: Terminal

  See also: std::CURSOR_END

  Example:

    $x ...
    get_key! $key
    case key
      CURSOR_HOME:
        dec &x
        next!
      ...

[source]

$std:DRAW_COLOUR .

[documentation remark]

# std::DRAW_COLOUR (a unique item)

  used to specifiy a draw colour

  Topic: Terminal

  See also: std_types::create/screen

[EXERCISE]

#
  Topic: AI

  Funky currently supports large language models based on the llama.cpp project.

  The "fllama" tool is a server program that must be started first before a
  client can send queries to the server.

  A short glossary:

  * An *LLM* is a Large Language Model.

  * A *modelname* is the name of an LLM in GGUF-format including the ".gguf"
    file extension.

  * A *model* is an object on the client side that represents an LLM.

  * A *token* is a non-negative integer number that represents one ore more
    (maybe partial) characters.

  * A *piece* is a raw octet string associated with a token. This string can
    contain partial UTF-8-sequences. Most models do not contain so many tokens
    to be able to encode all possible Unicode code points. So smileys might
    be represented by two tokens each describing a part of a single code point.

  * A *sequence* is a sequence of tokens stored on the server. Such a sequence
    contains prompts (supplied by the client) and answers (generated by the
    server). Sequences can be truncated, some tokens can be deleted or new
    tokens can be inserted. This might harm the quality of later evaluations
    because of rounding errors. Sequences can also be copied to share a common
    prefix string (e.g. some instructions) with each other.

  * A *sequence-id* is a client supplied non-negative integer used to identify
    a sequence.

  * *logits* are float values that describe how good a token is suited to
    continue a sequence. The higher the value the better the token represents
    the "inner state" of the model. Logit values are logarithmic, so a slightly
    higher value represents a much better match. The absolute value of logits
    are model dependent.

  * *tokenization* is the conversion of a text string into tokens.

  * *detokenization* is the conversion of tokens into a text stringl

  * *evaluation* is the generation of a list of logits to choose from for the
    next token based on the tokens already in the sequence

  Example:

    <require basic/stdlib>
    <require ai/new_llama>

    <using std>
    <using llama>

    $MODEL_NAME "sauerkrautlm-una-solar-instruct.Q5_K_M.gguf"
    $PROMPT "Once upon a time"

    $SEQ_ID 1
    open_model! $model MODEL_NAME
    tokenize! $tokens model PROMPT
    create_sequence! model SEQ_ID
    add_tokens! model SEQ_ID list(bos_token_of(model))
    add_tokens! model SEQ_ID tokens
    evaluate! model SEQ_ID
    $prefix ""
    print! PROMPT
    repeat 20:
      get_logits! $_seq_id $_position $logits model
      logits(1) $best_token $_best_logit
      detokenize model best_token $piece &prefix
      print! piece
      add_token_and_evaluate! model SEQ_ID best_token
      next!
    println!

  Output:

    Once upon a time, there was a little girl named Alice. Alice lived in a
    small village with her parents and her

[symbols]

$llama::TOKENS .
$llama::LOGITS .
$llama::ERROR .
$llama::fd_of ()
$llama::bos_token_of ()
$llama::eos_token_of ()
$llama::eot_token_of ()
$llama::sep_token_of ()
$llama::nl_token_of ()
$llama::pad_token_of ()
$llama_types::model std_types::object
$llama_types::model.llama::fd_of undefined
$llama_types::model.pieces_of undefined
$llama_types::model.llama::bos_token_of undefined
$llama_types::model.llama::eos_token_of undefined
$llama_types::model.llama::eot_token_of undefined
$llama_types::model.llama::sep_token_of undefined
$llama_types::model.llama::nl_token_of undefined
$llama_types::model.llama::pad_token_of undefined
$llama::detokenize:
$llama::list_models:
$llama::open_model ()
$std_types::string/open_model: (model_name)
$std_types::io/open_model: (io id model_name)
$std_types::io.model_of empty_hash_table
$std_types::io.read_buffer_of empty_hash_table
$llama_types::model/close: (model)
$llama::tokenize ()
$llama_types::model/tokenize: (model prompt)
$std_types::io/tokenize: (io model prompt)
$llama::create_sequence ()
$llama_types::model/create_sequence: (model sequence_id)
$std_types::io/create_sequence: (io model sequence_id)
$llama::copy_sequence ()
$llama_types::model/llama::copy_sequence:
$std_types::io/llama::copy_sequence:
$llama::delete_sequence ()
$llama_types::model/llama::delete_sequence: (model sequence_id)
$std_types::io/llama::delete_sequence: (io model sequence_id)
$llama::truncate_sequence ()
$llama_types::model/llama::truncate_sequence: (model sequence_id token_count)
$std_types::io/llama::truncate_sequence: (io model sequence_id token_count)
$llama::add_tokens ()
$llama_types::model/llama::add_tokens: (model sequence_id tokens)
$std_types::io/llama::add_tokens: (io model sequence_id tokens)
$llama::add_token_and_evaluate ()
$llama_types::model/llama::add_token_and_evaluate: (model sequence_id token)
$std_types::io/llama::add_token_and_evaluate: (io model sequence_id token)
$llama::delete_tokens ()
$llama_types::model/llama::delete_tokens:
$std_types::io/llama::delete_tokens:
$llama::insert_tokens ()
$llama_types::model/llama::insert_tokens:
$std_types::io/llama::insert_tokens:
$llama::evaluate ()
$llama_types::model/llama::evaluate:
$std_types::io/llama::evaluate:
$llama::get_logits: (model)

#
  Copyright (C) 2025 by
  Dipl.-Ing. Michael Niederle

  This program is free software; you can redistribute it and/or modify
  it under the terms of the GNU Library General Public License, version 2, or
  (at your option) under the terms of the GNU Lesser General Public License,
  version 3.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
  GNU Lesser (Library) General Public License for more details.

  For details of the GNU General Public License see the accompanying
  files LGPLv2.txt and LGLPv3.txt or
  http://www.gnu.org/licenses/lgpl-2.0.html
  http://www.gnu.org/licenses/lgpl-3.0.html
  or print to the
  Free Software Foundation, Inc.,
  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.

#
  Topic: AI

  Funky currently supports large language models based on the llama.cpp project.

  The "fllama" tool is a server program that must be started first before a
  client can send queries to the server.

  A short glossary:

  * An *LLM* is a Large Language Model.

  * A *modelname* is the name of an LLM in GGUF-format including the ".gguf"
    file extension.

  * A *model* is an object on the client side that represents an LLM.

  * A *token* is a non-negative integer number that represents one ore more
    (maybe partial) characters.

  * A *piece* is a raw octet string associated with a token. This string can
    contain partial UTF-8-sequences. Most models do not contain so many tokens
    to be able to encode all possible Unicode code points. So smileys might
    be represented by two tokens each describing a part of a single code point.

  * A *sequence* is a sequence of tokens stored on the server. Such a sequence
    contains prompts (supplied by the client) and answers (generated by the
    server). Sequences can be truncated, some tokens can be deleted or new
    tokens can be inserted. This might harm the quality of later evaluations
    because of rounding errors. Sequences can also be copied to share a common
    prefix string (e.g. some instructions) with each other.

  * A *sequence-id* is a client supplied non-negative integer used to identify
    a sequence.

  * *logits* are float values that describe how good a token is suited to
    continue a sequence. The higher the value the better the token represents
    the "inner state" of the model. Logit values are logarithmic, so a slightly
    higher value represents a much better match. The absolute value of logits
    are model dependent.

  * *tokenization* is the conversion of a text string into tokens.

  * *detokenization* is the conversion of tokens into a text stringl

  * *evaluation* is the generation of a list of logits to choose from for the
    next token based on the tokens already in the sequence

  Example:

    <require basic/stdlib>
    <require ai/new_llama>

    <using std>
    <using llama>

    $MODEL_NAME "sauerkrautlm-una-solar-instruct.Q5_K_M.gguf"
    $PROMPT "Once upon a time"

    $SEQ_ID 1
    open_model! $model MODEL_NAME
    tokenize! $tokens model PROMPT
    create_sequence! model SEQ_ID
    add_tokens! model SEQ_ID list(bos_token_of(model))
    add_tokens! model SEQ_ID tokens
    evaluate! model SEQ_ID
    $prefix ""
    print! PROMPT
    repeat 20:
      get_logits! $_seq_id $_position $logits model
      logits(1) $best_token $_best_logit
      detokenize model best_token $piece &prefix
      print! piece
      add_token_and_evaluate! model SEQ_ID best_token
      next!
    println!

  Output:

    Once upon a time, there was a little girl named Alice. Alice lived in a
    small village with her parents and her

<namespace llama>

<namespace llama_types>

<using std>

<using llama>

$llama::TOKENS .

$llama::LOGITS .

$llama::ERROR .

$llama::fd_of ()

$pieces_of ()

$llama::bos_token_of ()

$llama::eos_token_of ()

$llama::eot_token_of ()

$llama::sep_token_of ()

$llama::nl_token_of ()

$llama::pad_token_of ()

$llama_types::model std_types::object

$llama_types::model.llama::fd_of undefined

$llama_types::model.pieces_of undefined

$llama_types::model.llama::bos_token_of undefined

$llama_types::model.llama::eos_token_of undefined

$llama_types::model.llama::eot_token_of undefined

$llama_types::model.llama::sep_token_of undefined

$llama_types::model.llama::nl_token_of undefined

$llama_types::model.llama::pad_token_of undefined

$llama::detokenize:
  (
    model
    token_or_list
    prefix = ""
  )
  $rc result_count()
  $pieces pieces_of(model)
  if
    token_or_list.is_a_list:
      $text ""
      for_each token_or_list
        : (token)
          token_to_piece token $str &prefix
          append &text str
          next
        :
          if
            rc == 1
            -> text
            -> text prefix
    :
      token_to_piece token_or_list $text &prefix
      if
        rc == 1
        -> text
        -> text prefix

  $token_to_piece: (token prefix_string)
    $piece pieces(token+1)
    append prefix_string &piece
    $n length_of(piece)
    $i 1
    $e 0
    loop:
      update_if i <= n+1 &e -> i-1
      if
        i > n
        -> range(piece 1 e).from_utf8 range(piece e+1 -1)
        :
          $code piece(i).to_integer
          cond
            -> code < 0x80:
              inc &i
              next
            -> code & 0xe0 == 0xc0:
              plus &i 2
              next
            -> code & 0xf0 == 0xe0:
              plus &i 3
              next
            -> code & 0xf8 == 0xf0:
              plus &i 4
              next
            -> true -> "<???>" ""

$llama::list_models:
  open_tcp_client_socket! $fd "127.0.0.1" 7683
  write_to! fd "list_models@0;"
  handle_reply! $available_models fd handle_list_models
  close! fd
  -> available_models

  $handle_list_models: (_dummy reply)
    $tag reply .truncate_from. ' '
    case tag
      "models":
        $models split(behind(reply ' ' 2))
        -> models true
      "error":
        $message reply .behind. ' '
        -> error(message) true
      -> error("unexpected reply") true

$llama::open_model ()

$std_types::string/open_model: (model_name)
  open_tcp_client_socket! $fd "127.0.0.1" 7683 true
  write_to! fd "
    use_model @(model_name)@0;@
    get_pieces@0;@
    get_special_tokens@0;@
  handle_reply! $model fd handle_model_info
  if
    model.is_an_error:
      close! model
      -> model
    -> model(.fd_of fd)

$std_types::io/open_model: (io id model_name)
  run io open_model_request id model_name

$open_model_request: (io id model_name)
  open_tcp_client_socket! $fd "127.0.0.1" 7683 true
  write &io fd "
    use_model @(model_name)@0;@
    get_pieces@0;@
    get_special_tokens@0;@
  start_reading_from &io fd
  !io.model_of(id) llama_types::model(.fd_of fd)
  !io.read_buffer_of(fd) ""
  register_handlers &io fd
    WRITE_FAILED = tuple(write_failed id)
    READ = tuple(read_data id)
    READ_FAILED = tuple(read_failed id)
  -> io undefined

$handle_model_info: (model reply)
  update_if model.is_undefined &model -> llama_types::model
  $tag reply .truncate_from. ' '
  case tag
    "pieces":
      # ignore piece count
      $pieces split(behind(reply ' ' 2))
      map &pieces hex_to_string
      -> model(.pieces_of pieces) false
    "special_tokens":
      $special_tokens split(reply .behind. ' ')
      map &special_tokens: (str)
        key_value_pair
          str .before. '='
          to_integer(str .behind. '=')
      for_each special_tokens
        : (special_token)
          special_token $name $token
          if
            token >= 0:
              case name
                "bos":
                  !model.llama::bos_token_of token
                  next
                "eos":
                  !model.llama::eos_token_of token
                  next
                "eot":
                  !model.llama::eot_token_of token
                  next
                "sep":
                  !model.llama::sep_token_of token
                  next
                "nl":
                  !model.llama::nl_token_of token
                  next
                "pad":
                  !model.llama::pad_token_of token
                  next
                next
            next
        -> model true
    "error":
      $message reply .behind. ' '
      -> error(message) true
    -> error("unexpected reply") true

$model_of ()

$read_buffer_of ()

$std_types::io.model_of empty_hash_table

$std_types::io.read_buffer_of empty_hash_table

$write_failed: (io fd err id)
  !io.model_of(id) undefined
  !io.read_buffer_of(fd) undefined
  -> io tuple(JOB_FAILED id err)

$read_data: (io fd data id)
  $model model_of(io)(id)
  $reply read_buffer_of(io)(fd)
  loop:
    search $pos $_len '@nul;' data
    if
      pos.is_defined:
        append &reply range(data 1 pos-1)
        range &data pos+1 -1
        handle_model_info &model $done reply
        if
          done:
            !io.model_of(id) undefined
            !io.read_buffer_of(fd) ""
            deregister_all_handlers &io fd
            deregister_all_handlers &io id
            register_handlers &io fd
              WRITE_FAILED = request_failed
              READ = tuple(read_reply model)
              READ_FAILED = read_reply_failed
            -> io tuple(JOB_COMPLETED id model)
          :
            !reply ""
            next
      :
        append &reply data
        !io.model_of(id) model
        !io.read_buffer_of(fd) reply
        -> io undefined

$read_failed: (io _fd err id)
  debug::dump `err `id
  -> io undefined

$request_failed: (io fd err)
  !io.read_buffer_of(fd) undefined
  -> io tuple(ERROR fd err)

$read_reply: (io fd data model)
  $reply read_buffer_of(io)(fd)
  $events empty_list
  loop:
    search $pos $_len '@nul;' data
    if
      pos.is_defined:
        append &reply range(data 1 pos-1)
        range &data pos+1 -1
        $event create_reply_event(reply model)
        debug::dump 2 `event
        debug::exit
        #if
          done:
            !io.model_of(id) undefined
            !io.read_buffer_of(fd) ""
            deregister_all_handlers &io fd
            deregister_all_handlers &io id
            register_handlers &io fd
              WRITE_FAILED = request_failed
              READ = read_reply
              READ_FAILED = read_reply_failed
            -> io tuple(JOB_COMPLETED id result)
          :
            !reply ""
            next
      :
        append &reply data
        !io.read_buffer_of(fd) reply
        -> io events

$read_reply_failed: (io fd err)
  debug::dump `err `fd
  -> io undefined

$create_reply_event: (reply model)
  $tag reply .truncate_from. ' '
  case tag
    "tokens":
      $tokens map(split(reply .behind. ' ') to_integer)
      -> tuple(TOKENS undefined tokens model)
    "logits":
      $sequence_id between(reply ' ' ' ').to_integer
      $_position between(reply ' ' ' ' 2).to_integer
      $_count between(reply ' ' ' ' 3).to_integer
      $logits behind(reply ' ' 4)
      split &logits ' '
      map &logits: (str)
        key_value_pair
          to_integer(str .before. '=')
          to_number(str .behind. '=')
      -> tuple(LOGITS sequence_id logits model)
    "error":
      $message reply .behind. ' '
      -> tuple(ERROR undefined message model)
    -> tuple(ERROR undefined "invalid_reply" model)

$llama_types::model/close: (model)
  close! fd_of(model)

[source]

$llama::LOGITS .

[documentation remark]

# llama::LOGITS (a unique item)

