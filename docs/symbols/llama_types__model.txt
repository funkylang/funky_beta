# llama_types::model (a type)

  the prototype object for all models

  This object is never used directly but cloned to create a new model.

  The model contains a list of all *pieces* to allow for local detokenization.

  Tokenization on the other hand is done using complicated algorithms that
  depend on the model in use. Because a model was trained with exact this
  tokenization applied one should always rely on server-side tokenization!

  Topics: AI

  See also: llama::open_model

  Example:

    <require basic/stdlib>
    <require ai/new_llama>

    <using std>
    <using llama>

    $MODEL_NAME "sauerkrautlm-una-solar-instruct.Q5_K_M.gguf"
    $PROMPT "Once upon a time"

    $SEQ_ID 1
    open_model! $model MODEL_NAME
    tokenize! $tokens model PROMPT
    create_sequence! model SEQ_ID
    add_tokens! model SEQ_ID list(bos_token_of(model))
    add_tokens! model SEQ_ID tokens
    evaluate! model SEQ_ID
    $prefix ""
    print! PROMPT
    repeat 20:
      get_logits! $_seq_id $_position $logits model
      logits(1) $best_token $_best_logit
      detokenize model best_token $piece &prefix
      print! piece
      add_token_and_evaluate! model SEQ_ID best_token
      next!
    println!

((defined in ai/new_llama.fky))
((generated by Codestral-22B-v0.1-Q5_K_M.gguf))
((2025-04-02 10:50:58))
