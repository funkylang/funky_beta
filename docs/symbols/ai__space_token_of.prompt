[MANUAL]

In Funky function calls are either written on a single line with arguments
enclosed within parentheses and separated by single spaces, e.g.:

print! f(a b c) g(x y z)

or they are written in indentation style using mutliple lines:

print!
  f
    a
    b
    c
  g
    x
    y
    z

Functions that create any output or read any input need I/O-access rights.

A function that needs I/O-access rights must be called with a trailing
exclamation mark after the function name, e.g.:

println! "Hello, World!"

The expression

key = value

denotes a key-value-pair.

Collection types cannot be printed directly. You can use "dump!" to
dump the contents of a collection, e.g.:

$ages
  hash_table
    "John" = 42
    "Mary" = 25
    "Irene" = 49

dump! `ages

Output:

ages: hash_table <6>
  "Mary"
  25
  "Irene"
  49
  "John"
  42

Note: The number in angular brackets after the typename denotes the number
of (indented) lines that follow the head.

A tail call returns the result of the called function; e.g.:

$calc:
  (
    x
    y
  )
  plus x*x y*y

println! calc(2 3)

prints

13

[GUIDELINES]

All documentation remarks must have

* a single short (single line) description written in lower case

* a parameter(s) section (if appropriate)

* a result(s) section (if appropriate)

* a detailed description of how to use the symbol: when describing unique
  items of constants, describe how to use them;
  do not repeat yourself

* one or more associated topic(s) (if appropriate)

* a "See also" entry - mention only a view relevant symbols here

Do not mention unknown symbols!

Do not mention more than 10 symbols in the "See also" entry.

DO NOT MENTION MORE THAN 10 SYMBOLS IN THE "See also" ENTRY!

Do not mention internal details of functions.

Do not start each sentence in the description with "If" or "The".

DO NOT START EACH SENTENCE IN THE DESCRIPTION WITH "If" OR "The"!

Do not tell the user what a function does not do.

When referring to another symbol write it enclosed in angle brackets, e.g.
<std::clear>.

The "self"-parameter is a normal parameter like any other and should be
mentioned in the the parameter description.

Replace symbolic constants by their value.

Operations on I/O-objects are not executed immediately but queued as jobs.
To identify a job, an "id" is used.

I/O-functions and I/O-methods either need I/O-access rights or propagate
them ("if", "for_each", etc.).

An "example" section, if available, must use Funky syntax and shall not be too
long.

If there is an appropriate example in the source code then use this example
in the "example" section!

[EXAMPLES]

[source]

$std::clear_colour_of ()

[documentation remark]

# std::clear_colour_of (a polymorphic function)

  the clear colour to use

  Parameter:
    self: the object from which to get the clear colour

  Result:
    colour: the clear colour

  Returns the clear colour of the object.

  Topic: Terminal

  See also: std::draw_colour_of, std::set_clear_colour

[source]

$std_types::screen/clear:
  (
    self
    x = 1
    y = 1
    width = undefined
    height = undefined
  )
  update_if width.is_undefined &width -> width_of(self)-x+1
  update_if height.is_undefined &height -> height_of(self)-y+1
  std_private::clear self self x y width height

$std::clear ()

[documentation remark]

# std::clear (a polymorphic function)

  clears a rectangular area

  Parameters:
    screen # some kind of screen
    x: x-coordinate of the upper left corner
    y: y-coordinate of the upper left corner
    width: width of the area
    height: height of the area

  Result:
    screen: the updated screen

  The area is cleared using the current clear colour. Use
  <std::set_clear_colour> to change the current clear colour.

  The area is clipped at the screen's borders.

  Topic: Terminal

  See also: std_types::screen/std::clear, std::draw_text,
            std::set_clear_colour

  Example:

    $my_screen create_screen(terminal 40 25)
    set_clear_colour &my_screen GREEN
    clear &my_screen 11 6 20 15

[EXERCISE]

#
  Topic: AI

  Funky currently supports large language models based on the llama.cpp project.

  A "piece" is the string associated with a token.

  Example:

    <require basic/stdlib>
    <require basic/uuid>
    <require basic/export/json>
    <require basic/import/json>
    <require ai/llama>
    <require web/server>

    generate_uuid! $uuid undefined
    ai::load_ai_model! $model modelname # assumes an already running AI-server
      UUID = uuid
      ai::CONTEXT_SIZE = 4096
    print! prompt
    ai::tokenize! $tokens model prompt
    repeat 10 # generate 10 tokens
      :
        ai::evaluate! $token model tokens
        $piece ai::detokenize(model token)
        print! piece
        push &tokens token
        next!
      :
        ai::deregister! uuid # deregister from the AI-server

    $modelname "sauerkrautlm-una-solar-instruct.Q5_K_M.gguf"
    $prompt "Once upon a time"

  Output:

    Once upon a time, there was a beautiful princess named Aurora.

[symbols]


# Modes

$ai::EXACT . # evaluate the result using the full context size (slow)
$ai::SMART .

# Attributes

$ai::pieces_of ()
$ai::piece_table_of ()
$ai::maximum_piece_length_of ()
$ai::begin_of_stream_token_of ()
$ai::end_of_stream_token_of ()
$ai::classsification_token_of ()
$ai::separator_token_of ()
$ai::newline_token_of ()
$ai::space_token_of ()
$ai::end_of_text_token_of ()
$ai::prefix_token_of ()
$ai::suffix_token_of ()
$ai::middle_token_of ()
$ai::question_token_of ()
$ai::answer_token_of ()
$ai::context_size_of ()
$ai::effective_context_size_of ()
$std::address_of ()
$std::port_no_of ()

# Methods

$ai::evaluate ()
$ai::tokenize ()
$ai::detokenize ()
$ai::generate ()
$ai::deregister ()

# Options

$ai::SERVER_NAME .
$ai::CONTEXT_SIZE .
$ai::MODEL_PATH .
$ai::MINIMUM_CONFIDENCE .
$ai::MAXIMUM_LENGTH .
$ai::STOP .
$ai::BACKTRACK .
$ai::LOG_LEVEL .
$ai::VERBOSE .
$ai::USE_COLOURS .

# Prototype Objects

$ai_types::connection std_types::resource
$std_types::object.is_a_connection false
$ai_types::connection.is_a_connection true
$ai_types::connection.address_of undefined
$ai_types::connection.port_no_of undefined
$ai_types::connection.uuid_of undefined
$ai_types::connection.name_of "connection"
$ai_types::connection/finalize:
$ai_types::model std_types::object

# Implementation

$ai::list_ai_models:
$ai::load_ai_model ()
$std_types::string/ai::load_ai_model:
$std_types::io.model_of empty_hash_table
$std_types::io/ai::load_ai_model:
$ai_types::model/ai::generate:
$ai_types::model/ai::tokenize:
$std_types::io/ai::tokenize:
$ai_types::model/ai::evaluate:
$std_types::io/ai::evaluate:
$ai_types::model/ai::detokenize:
$ai::connect_to_ai_server:
$std_types::io.connection_of empty_hash_table
$std_types::string/ai::deregister:
$ai_types::model/ai::deregister:
$std_types::io/ai::deregister:
$ai::escaped_token: (model token)
$ai::escaped_piece: (piece)
$ai::start_ai_server: (io id options*)

#
  Copyright (C) 2023 by
  Dipl.-Ing. Michael Niederle

  This program is free software; you can redistribute it and/or modify
  it under the terms of the GNU Library General Public License, version 2, or
  (at your option) under the terms of the GNU Lesser General Public License,
  version 3.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
  GNU Lesser (Library) General Public License for more details.

  For details of the GNU General Public License see the accompanying
  files LGPLv2.txt and LGLPv3.txt or
  http://www.gnu.org/licenses/lgpl-2.0.html
  http://www.gnu.org/licenses/lgpl-3.0.html
  or print to the
  Free Software Foundation, Inc.,
  51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.

<namespace ai>

<namespace ai_types>

#
  Topic: AI

  Funky currently supports large language models based on the llama.cpp project.

  A "piece" is the string associated with a token.

  Example:

    <require basic/stdlib>
    <require basic/uuid>
    <require basic/export/json>
    <require basic/import/json>
    <require ai/llama>
    <require web/server>

    generate_uuid! $uuid undefined
    ai::load_ai_model! $model modelname # assumes an already running AI-server
      UUID = uuid
      ai::CONTEXT_SIZE = 4096
    print! prompt
    ai::tokenize! $tokens model prompt
    repeat 10 # generate 10 tokens
      :
        ai::evaluate! $token model tokens
        $piece ai::detokenize(model token)
        print! piece
        push &tokens token
        next!
      :
        ai::deregister! uuid # deregister from the AI-server

    $modelname "sauerkrautlm-una-solar-instruct.Q5_K_M.gguf"
    $prompt "Once upon a time"

  Output:

    Once upon a time, there was a beautiful princess named Aurora.

# Modes

$ai::EXACT . # evaluate the result using the full context size (slow)

$ai::SMART .

$DEFAULT_PORT_NO 8080

# Attributes

$ai::pieces_of ()

$ai::piece_table_of ()

$ai::maximum_piece_length_of ()

$ai::begin_of_stream_token_of ()

$ai::end_of_stream_token_of ()

$ai::classsification_token_of ()

$ai::separator_token_of ()

$ai::newline_token_of ()

$ai::space_token_of ()

$ai::end_of_text_token_of ()

$ai::prefix_token_of ()

$ai::suffix_token_of ()

$ai::middle_token_of ()

$ai::question_token_of ()

$ai::answer_token_of ()

$ai::context_size_of ()

$ai::effective_context_size_of ()
  #
    the context size to effectively use with the model

    must be less or equal to <ai::context_size_of> (or <std::undefined>)

$std::address_of ()

$std::port_no_of ()

$uuid_of ()

# Methods

$ai::evaluate ()

$ai::tokenize ()

$ai::detokenize ()

$ai::generate ()

$ai::deregister ()

# Options

$ai::SERVER_NAME .

$ai::CONTEXT_SIZE .

$ai::MODEL_PATH .

$ai::MINIMUM_CONFIDENCE .

$ai::MAXIMUM_LENGTH .

$ai::STOP .

$ai::BACKTRACK .

$ai::LOG_LEVEL .

$ai::VERBOSE .

$ai::USE_COLOURS .

# Prototype Objects

$ai_types::connection std_types::resource

$is_a_connection ()

$std_types::object.is_a_connection false

$ai_types::connection.is_a_connection true

$ai_types::connection.address_of undefined

$ai_types::connection.port_no_of undefined

$ai_types::connection.uuid_of undefined

$ai_types::connection.name_of "connection"

$DEREGISTER .

$ai_types::connection/finalize:
  (
    self
    io
  )
  $address address_of(self)
  $port_no port_no_of(self)
  $uuid uuid_of(self)
  $id tuple(DEREGISTER uuid)
  log &io 3 "deregistering from AI server at address @(address):@(port_no)"
  http_request! &io $_events id address port_no deregister_request(uuid)
  register_handlers io id
    JOB_COMPLETED = deregister_completed
    JOB_FAILED = deregister_failed

$deregister_completed: (io id _data)
  log &io 3 "successfully deregistered from AI server"
  deregister_all_handlers &io id
  -> io undefined

$deregister_failed: (io id _data)
  log &io "failed to deregister from AI server"
  deregister_all_handlers &io id
  -> io undefined

$ai_types::model std_types::object

# Implementation

$ai::list_ai_models:
  (
     options*
  )
  extract_options options
    ADDRESS = "127.0.0.1" $address
    PORT_NO = DEFAULT_PORT_NO $port_no
    UUID = undefined $uuid
  $json
    to_json
      insert_order_table
        "uuid" = uuid
  to_utf8 &json
  open_tcp_client_socket! $fd address port_no
  write_to! fd "
    POST /list_models HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)
  load! $result fd
  on result.is_an_error result
  behind &result "@cr;@nl;@cr;@nl;"
  from_utf8 &result
  $info result.from_json
  -> info("models")

$ai::load_ai_model ()

$std_types::string/ai::load_ai_model:
  (
    name
    options*
  )
  initialize_model $model name options
  get_tokens! model
    !model.ai::pieces_of
    !model.ai::begin_of_stream_token_of
    !model.ai::end_of_stream_token_of
    !model.ai::classsification_token_of
    !model.ai::separator_token_of
    !model.ai::newline_token_of
    !model.ai::end_of_text_token_of
    !model.ai::prefix_token_of
    !model.ai::suffix_token_of
    !model.ai::middle_token_of
    !model.ai::context_size_of
  !model
    model
      .ai::question_token_of undefined
      .ai::answer_token_of undefined
  if
    ai::pieces_of(model).is_an_error
    -> ai::pieces_of(model)
    :
      build_piece_table &model

      #
        The default special tokens are only meant to be used with LLama models
        So let's replace them for other models (especially Falcon).

      !model.ai::newline_token_of ai::piece_table_of(model)("@nl;")
      !model.ai::space_token_of ai::piece_table_of(model)(" ")

      for_each ai::pieces_of(model)
        : (token piece)
          if
            &&
              piece.is_a_string
              length_of(piece) > 4
              ||
                piece .has_prefix. "<|" && piece .has_suffix. "|>"
                piece .has_prefix. ">>" && piece .has_suffix. "<<"
            :
              $special range(piece 3 -3)
              case special
                "endoftext":
                  !model.ai::end_of_text_token_of token-1
                  next
                "QUESTION":
                  !model.ai::question_token_of token-1
                  next
                "ANSWER":
                  !model.ai::answer_token_of token-1
                  next
                "PREFIX":
                  !model.ai::prefix_token_of token-1
                  next
                "SUFFIX":
                  !model.ai::suffix_token_of token-1
                  next
                "MIDDLE":
                  !model.ai::middle_token_of token-1
                  next
                next
            next
        -> model

$model_of ()

$std_types::io.model_of empty_hash_table

$std_types::io/ai::load_ai_model:
  (
    io
    id
    name
    options*
  )
  update_if
    &&
      length_of(options) == 1
      ||
        options(1).is_a_list
        options(1).is_a_connection
    &options -> options(1)
  initialize_model $model name options
  tuple &id 1
  run &io http_request id
    address_of(model) port_no_of(model) get_tokens_request(model)
  !io.model_of(id) model
  register_handlers io id
    JOB_COMPLETED = load_ai_model_completed
    JOB_FAILED = load_ai_model_failed

$load_ai_model_completed: (io id data)
  $model model_of(io)(id)
  !io.model_of(id) undefined
  deregister_all_handlers &io id
  id !id
  from_utf8 &data
  from_json &data
  if
    data.is_an_error || data.is_undefined
    -> io tuple(JOB_FAILED id error(IO_ERROR "INVALID JSON"))
    :
      !model
        .ai::pieces_of data("tokens")
        .ai::begin_of_stream_token_of data("begin_of_stream")
        .ai::end_of_stream_token_of data("end_of_stream")
        .ai::classsification_token_of data("classification")
        .ai::separator_token_of data("separator")
        .ai::newline_token_of data("newline")
        .ai::end_of_text_token_of data("end_of_text")
        .ai::prefix_token_of data("prefix")
        .ai::suffix_token_of data("suffix")
        .ai::middle_token_of data("middle")
        .ai::context_size_of data("context_size")
      -> io tuple(JOB_COMPLETED id model)

$load_ai_model_failed: (io id err)
  !io.model_of(id) undefined
  deregister_all_handlers &io id
  id !id
  -> io tuple(JOB_FAILED id err)

$initialize_model: (name options)
  if
    options.is_a_connection
    ->
      ai_types::model
        .name_of name
        .address_of address_of(options)
        .port_no_of port_no_of(options)
        .uuid_of uuid_of(options)
        .ai::effective_context_size_of undefined
    :
      extract_options options
        ADDRESS = "127.0.0.1" $address
        PORT_NO = DEFAULT_PORT_NO $port_no
        UUID = undefined $uuid
        ai::CONTEXT_SIZE = undefined $context_size
      ->
        ai_types::model
          .name_of name
          .address_of address
          .port_no_of port_no
          .uuid_of uuid
          .ai::effective_context_size_of context_size

$ai_types::model/ai::generate:
  (
    model
    options*
    prompt
  )
  extract_options options
    ai::MINIMUM_CONFIDENCE = 15 $minimum_confidence
    ai::MAXIMUM_LENGTH = 1024 $maximum_length
    ai::STOP = undefined $stop_text
    ai::BACKTRACK = 0 $backtrack
    ai::LOG_LEVEL = 0 $log_level
    ai::VERBOSE = false $be_verbose
    ai::USE_COLOURS = false $use_colours
  $do_log log_level > 0
  update_if use_colours &be_verbose -> true
  on use_colours: print! ansi_text_colour(BLACK)
  ai::tokenize! $tokens model prompt
  generate_text! $text $_max_no tokens "" 0 empty_list
  on use_colours: print! ansi_reset_colour()
  on be_verbose: println!
  if
    result_count() == 1
    -> text
    pass

  $generate_text: (tokens text no prefix)
    if
      stop_text.is_defined && text .contains. stop_text
      -> text no
      :
        current_time! $t1
        ai::evaluate! model tokens $next_token $confidence_values
        if
          next_token == ai::end_of_stream_token_of(model):
            on do_log:
              eprintln! "<end of stream>"
            -> text no
          :
            $max_no no
            loop:
              confidence_values(1) !next_token $next_confidence
              ai::detokenize model next_token prefix $next_piece $suffix
              on do_log:
                current_time! $t2
                $dt 1000*(t2-t1) # in ms
                if
                  log_level <= 1:
                    $token_string
                      "@quot;@(ai::escaped_piece(next_piece))@quot;:"
                    eprintln!
                      format
                        "[#%4] %l18 %3.3"
                        no token_string next_confidence
                  :
                    if
                      log_level >= 3:
                        eprintln! format("-[#%4|%3.1 ms]---------------" no dt)
                      :
                        eprintln! format("-[#%4]------------------------" no dt)
                    for_each confidence_values
                      : (confidence_value)
                        confidence_value $token $confidence
                        ai::detokenize model token prefix $piece
                        replace_all &piece '@ht;' = "<<<ht>>>"
                        $token_string "@quot;@(ai::escaped_piece(piece))@quot;:"
                        eprintln!
                          format
                            "%5 %l18 %3.3" token token_string confidence
                        next!
                      pass
              if
                next_confidence >= minimum_confidence || no < 5:
                  update_if no == 0 &next_piece: trim_left next_piece
                  print_piece! next_piece next_confidence
                  if
                    no >= maximum_length
                    -> append(text next_piece) no
                    :
                      generate_text! $response $highest_no
                        push(tokens next_token)
                        append(text next_piece)
                        no+1
                        suffix
                      max &max_no highest_no
                      if
                        response.is_empty:
                          if
                            next_piece == "@nl;"
                            -> push(text '@nl;') max_no # stop backtracking
                            :
                              if
                                ||
                                  length_of(confidence_values) < 2
                                  max_no > no+backtrack
                                -> "" max_no
                                :
                                  on use_colours: print! ansi_reset_colour()
                                  on be_verbose: print!
                                    dup("@bs; @bs;" length_of(next_piece))
                                  on use_colours: print! ansi_text_colour(BLACK)
                                  range &confidence_values 2 -1
                                  next!
                        -> response max_no
                -> "" max_no

  $print_piece: (piece confidence)
    on use_colours:
      $confidence_delta confidence-minimum_confidence
      $colour
        cond
          -> confidence_delta < 0 ->
            colour_mixture
              BLACK = -confidence_delta
              RED = minimum_confidence+confidence_delta
          -> confidence_delta > 10 ->
            colour_mixture
              GREEN = 10
              WHITE = confidence_delta-10
          -> true ->
            colour_mixture
              RED = 10-confidence_delta
              GREEN = confidence_delta
      print! ansi_background_colour(colour)
    on be_verbose: print! piece

$get_tokens: (model)
  $request get_tokens_request(model)
  open_tcp_client_socket! $fd address_of(model) port_no_of(model)
  print_to! fd request
  load! $result fd
  $error_code between(result ' ' ' ').to_integer
  if
    error_code >= 300:
      error result .behind. "@cr;@nl;@cr;@nl;"
    :
      behind &result "@cr;@nl;@cr;@nl;"
      from_utf8 &result
      $info result.from_json
      ->
        info("tokens")
        info("begin_of_stream")
        info("end_of_stream")
        info("classification")
        info("separator")
        info("newline")
        info("end_of_text")
        info("prefix")
        info("suffix")
        info("middle")
        info("context_size")

$get_tokens_request: (model)
  $options
    insert_order_table
      "model" = name_of(model)
      "uuid" = uuid_of(model)
  update_if ai::effective_context_size_of(model).is_defined &options
    -> options("context_size" ai::effective_context_size_of(model))
  $json options.to_json
  to_utf8 &json
  -> "
    POST /get_tokens HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)@

$ai_types::model/ai::tokenize:
  (
    model
    text
  )
  $request tokenize_request(model text)
  open_tcp_client_socket! $fd address_of(model) port_no_of(model)
  write_to! fd request
  load! $result fd
  behind &result "@cr;@nl;@cr;@nl;"
  from_utf8 &result
  $info result.from_json
  -> info("tokens")

$std_types::io/ai::tokenize:
  (
    io
    id
    model
    text
  )
  tuple &id 1
  run &io http_request id
    address_of(model) port_no_of(model) tokenize_request(model text)
  register_handlers io id
    JOB_COMPLETED = tokenize_completed
    JOB_FAILED = tokenize_failed

$tokenize_completed: (io id data)
  deregister_all_handlers &io id
  id !id
  from_utf8 &data
  from_json &data
  if
    data.is_an_error || data.is_undefined
    -> io tuple(JOB_FAILED id error(IO_ERROR "INVALID JSON"))
    -> io tuple(JOB_COMPLETED id data("tokens"))

$tokenize_failed: (io id err)
  deregister_all_handlers &io id
  id !id
  -> io tuple(JOB_FAILED id err)

[source]

$ai::space_token_of ()

[documentation remark]

### Always copy examples contained in the symbol's description remark
    into the documentation

### Do not create completely new examples on your own

### Use only one of the following topics: AI

# ai::space_token_of (a polymorphic function)

