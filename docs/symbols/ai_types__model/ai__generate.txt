# ai_types::model/ai::generate (an I/O-method)

  generates text based on the specified prompt

  Parameters:
    model: the model to use
    options*: the options to use
    prompt: the prompt to use

  Results:
    the generated text

  **Attention**: This function must be called with I/O-access rights!

  The options are:

    ai::MINIMUM_CONFIDENCE: the minimum confidence for a token to be accepted
    ai::MAXIMUM_LENGTH: the maximum length of the generated text
    ai::STOP: the text to stop generating at
    ai::BACKTRACK: the number of tokens to backtrack if the confidence is too low
    ai::LOG_LEVEL: the log level to use
    ai::VERBOSE: whether to print the generated text in verbose mode
    ai::USE_COLOURS: whether to use colours to indicate the confidence

  The default values are:

    ai::MINIMUM_CONFIDENCE: 15
    ai::MAXIMUM_LENGTH: 1024
    ai::STOP: undefined
    ai::BACKTRACK: 0
    ai::LOG_LEVEL: 0
    ai::VERBOSE: false
    ai::USE_COLOURS: false

  The prompt is tokenized and then the tokens are evaluated using the model.
  The result is a list of tuples containing the token and the confidence.
  The token with the highest confidence is selected and appended to the
  generated text. This process is repeated until the maximum length is
  reached or the stop text is found.

  Topic: Deprecated AI

  See also: ai::evaluate, ai::tokenize, ai::detokenize

((defined in ai/llama.fky))
((generated by Codestral-22B-v0.1-Q5_K_M.gguf))
((2024-07-03 16:01:07))
