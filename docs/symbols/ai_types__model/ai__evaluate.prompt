[MANUAL]

In Funky function calls are either written on a single line with arguments
enclosed within parentheses and separated by single spaces, e.g.:

print! f(a b c) g(x y z)

or they are written in indentation style using mutliple lines:

print!
  f
    a
    b
    c
  g
    x
    y
    z

Functions that create any output or read any input need I/O-access rights.

A function that needs I/O-access rights must be called with a trailing
exclamation mark after the function name, e.g.:

println! "Hello, World!"

The expression

key = value

denotes a key-value-pair.

Collection types cannot be printed directly. You can use "dump!" to
dump the contents of a collection, e.g.:

$ages
  hash_table
    "John" = 42
    "Mary" = 25
    "Irene" = 49

dump! `ages

Output:

ages: hash_table <6>
  "Mary"
  25
  "Irene"
  49
  "John"
  42

Note: The number in angular brackets after the typename denotes the number
of (indented) lines that follow the head.

A tail call returns the result of the called function; e.g.:

$calc:
  (
    x
    y
  )
  plus x*x y*y

println! calc(2 3)

prints

13

[GUIDELINES]

All documentation remarks must have

* a single short (single line) description written in lower case

* a parameter(s) section (if appropriate)

* a result(s) section (if appropriate)

* a detailed description of how to use the symbol: when describing unique
  items of constants, describe how to use them;
  do not repeat yourself

* one or more associated topic(s) (if appropriate)

* a "See also" entry - mention only a view relevant symbols here

Do not mention unknown symbols!

Do not mention more than 10 symbols in the "See also" entry.

DO NOT MENTION MORE THAN 10 SYMBOLS IN THE "See also" ENTRY!

Do not mention internal details of functions.

Do not start each sentence in the description with "If" or "The".

DO NOT START EACH SENTENCE IN THE DESCRIPTION WITH "If" OR "The"!

Do not tell the user what a function does not do.

When referring to another symbol write it enclosed in angle brackets, e.g.
<std::clear>.

The "self"-parameter is a normal parameter like any other and should be
mentioned in the the parameter description.

Replace symbolic constants by their value.

Operations on I/O-objects are not executed immediately but queued as jobs.
To identify a job, an "id" is used.

I/O-functions and I/O-methods either need I/O-access rights or propagate
them ("if", "for_each", etc.).

An "example" section, if available, must use Funky syntax and shall not be too
long.

If there is an appropriate example in the source code then use this example
in the "example" section!

[EXAMPLES]

[symbols]

$std::write_string_to:
$std::write_to:
$std::writeln_to:
$std::print_string_to:
$std::print_to ()
$std_types::file_descriptor/print_to:
$std::println_to:
$std::print_string:
$std::print:
$std::println:

[source]

$std_types::file_descriptor/print_to:
  (
    fd
    args*
  )

[documentation remark]

# std_types::file_descriptor/std::print_to (an I/O-method)

  prints the specified arguments to the specified file descriptor

  Parameters:
    fd: the file descriptor to print to
    args*: the arguments to print

  Results:
    none

  **Attention**: This function must be called with I/O-access rights!

  The arguments are converted to strings, concatenated and written to the
  specified file descriptor.

  Topic: Input-Output

  See also: std::println_to, std::print_string_to

[EXERCISE]

#
  Topic: AI

  Funky currently supports large language models based on the llama.cpp project.

  A "piece" is the string associated with a token.

  Example:

    <require basic/stdlib>
    <require basic/uuid>
    <require basic/export/json>
    <require basic/import/json>
    <require ai/llama>
    <require web/server>

    generate_uuid! $uuid undefined
    ai::load_ai_model! $model modelname # assumes an already running AI-server
      UUID = uuid
      ai::CONTEXT_SIZE = 4096
    print! prompt
    ai::tokenize! $tokens model prompt
    repeat 10 # generate 10 tokens
      :
        ai::evaluate! $token model tokens
        $piece ai::detokenize(model token)
        print! piece
        push &tokens token
        next!
      :
        ai::deregister! uuid # deregister from the AI-server

    $modelname "sauerkrautlm-una-solar-instruct.Q5_K_M.gguf"
    $prompt "Once upon a time"

  Output:

    Once upon a time, there was a beautiful princess named Aurora.

[symbols]


# Modes

$ai::EXACT . # evaluate the result using the full context size (slow)
$ai::SMART .

# Attributes

$ai::pieces_of ()
$ai::piece_table_of ()
$ai::maximum_piece_length_of ()
$ai::begin_of_stream_token_of ()
$ai::end_of_stream_token_of ()
$ai::classsification_token_of ()
$ai::separator_token_of ()
$ai::newline_token_of ()
$ai::space_token_of ()
$ai::end_of_text_token_of ()
$ai::prefix_token_of ()
$ai::suffix_token_of ()
$ai::middle_token_of ()
$ai::question_token_of ()
$ai::answer_token_of ()
$ai::context_size_of ()
$ai::effective_context_size_of ()
$std::address_of ()
$std::port_no_of ()

# Methods

$ai::evaluate ()
$ai::tokenize ()
$ai::detokenize ()
$ai::generate ()
$ai::deregister ()

# Options

$ai::SERVER_NAME .
$ai::CONTEXT_SIZE .
$ai::MODEL_PATH .
$ai::MINIMUM_CONFIDENCE .
$ai::MAXIMUM_LENGTH .
$ai::STOP .
$ai::BACKTRACK .
$ai::LOG_LEVEL .
$ai::VERBOSE .
$ai::USE_COLOURS .

# Prototype Objects

$ai_types::connection std_types::resource
$std_types::object.is_a_connection false
$ai_types::connection.is_a_connection true
$ai_types::connection.address_of undefined
$ai_types::connection.port_no_of undefined
$ai_types::connection.uuid_of undefined
$ai_types::connection.name_of "connection"
$ai_types::connection/finalize:
$ai_types::model std_types::object

# Implementation

$ai::list_ai_models:
$ai::load_ai_model ()
$std_types::string/ai::load_ai_model:
$std_types::io.model_of empty_hash_table
$std_types::io/ai::load_ai_model:
$ai_types::model/ai::generate:
$ai_types::model/ai::tokenize:
$std_types::io/ai::tokenize:
$ai_types::model/ai::evaluate:
$std_types::io/ai::evaluate:
$ai_types::model/ai::detokenize:
$ai::connect_to_ai_server:
$std_types::io.connection_of empty_hash_table
$std_types::string/ai::deregister:
$ai_types::model/ai::deregister:
$std_types::io/ai::deregister:
$ai::escaped_token: (model token)
$ai::escaped_piece: (piece)
$ai::start_ai_server: (io id options*)

$load_ai_model_completed: (io id data)
  $model model_of(io)(id)
  !io.model_of(id) undefined
  deregister_all_handlers &io id
  id !id
  from_utf8 &data
  from_json &data
  if
    data.is_an_error || data.is_undefined
    -> io tuple(JOB_FAILED id error(IO_ERROR "INVALID JSON"))
    :
      !model
        .ai::pieces_of data("tokens")
        .ai::begin_of_stream_token_of data("begin_of_stream")
        .ai::end_of_stream_token_of data("end_of_stream")
        .ai::classsification_token_of data("classification")
        .ai::separator_token_of data("separator")
        .ai::newline_token_of data("newline")
        .ai::end_of_text_token_of data("end_of_text")
        .ai::prefix_token_of data("prefix")
        .ai::suffix_token_of data("suffix")
        .ai::middle_token_of data("middle")
        .ai::context_size_of data("context_size")
      -> io tuple(JOB_COMPLETED id model)

$load_ai_model_failed: (io id err)
  !io.model_of(id) undefined
  deregister_all_handlers &io id
  id !id
  -> io tuple(JOB_FAILED id err)

$initialize_model: (name options)
  if
    options.is_a_connection
    ->
      ai_types::model
        .name_of name
        .address_of address_of(options)
        .port_no_of port_no_of(options)
        .uuid_of uuid_of(options)
        .ai::effective_context_size_of undefined
    :
      extract_options options
        ADDRESS = "127.0.0.1" $address
        PORT_NO = DEFAULT_PORT_NO $port_no
        UUID = undefined $uuid
        ai::CONTEXT_SIZE = undefined $context_size
      ->
        ai_types::model
          .name_of name
          .address_of address
          .port_no_of port_no
          .uuid_of uuid
          .ai::effective_context_size_of context_size

$ai_types::model/ai::generate:
  (
    model
    options*
    prompt
  )
  extract_options options
    ai::MINIMUM_CONFIDENCE = 15 $minimum_confidence
    ai::MAXIMUM_LENGTH = 1024 $maximum_length
    ai::STOP = undefined $stop_text
    ai::BACKTRACK = 0 $backtrack
    ai::LOG_LEVEL = 0 $log_level
    ai::VERBOSE = false $be_verbose
    ai::USE_COLOURS = false $use_colours
  $do_log log_level > 0
  update_if use_colours &be_verbose -> true
  on use_colours: print! ansi_text_colour(BLACK)
  ai::tokenize! $tokens model prompt
  generate_text! $text $_max_no tokens "" 0 empty_list
  on use_colours: print! ansi_reset_colour()
  on be_verbose: println!
  if
    result_count() == 1
    -> text
    pass

  $generate_text: (tokens text no prefix)
    if
      stop_text.is_defined && text .contains. stop_text
      -> text no
      :
        current_time! $t1
        ai::evaluate! model tokens $next_token $confidence_values
        if
          next_token == ai::end_of_stream_token_of(model):
            on do_log:
              eprintln! "<end of stream>"
            -> text no
          :
            $max_no no
            loop:
              confidence_values(1) !next_token $next_confidence
              ai::detokenize model next_token prefix $next_piece $suffix
              on do_log:
                current_time! $t2
                $dt 1000*(t2-t1) # in ms
                if
                  log_level <= 1:
                    $token_string
                      "@quot;@(ai::escaped_piece(next_piece))@quot;:"
                    eprintln!
                      format
                        "[#%4] %l18 %3.3"
                        no token_string next_confidence
                  :
                    if
                      log_level >= 3:
                        eprintln! format("-[#%4|%3.1 ms]---------------" no dt)
                      :
                        eprintln! format("-[#%4]------------------------" no dt)
                    for_each confidence_values
                      : (confidence_value)
                        confidence_value $token $confidence
                        ai::detokenize model token prefix $piece
                        replace_all &piece '@ht;' = "<<<ht>>>"
                        $token_string "@quot;@(ai::escaped_piece(piece))@quot;:"
                        eprintln!
                          format
                            "%5 %l18 %3.3" token token_string confidence
                        next!
                      pass
              if
                next_confidence >= minimum_confidence || no < 5:
                  update_if no == 0 &next_piece: trim_left next_piece
                  print_piece! next_piece next_confidence
                  if
                    no >= maximum_length
                    -> append(text next_piece) no
                    :
                      generate_text! $response $highest_no
                        push(tokens next_token)
                        append(text next_piece)
                        no+1
                        suffix
                      max &max_no highest_no
                      if
                        response.is_empty:
                          if
                            next_piece == "@nl;"
                            -> push(text '@nl;') max_no # stop backtracking
                            :
                              if
                                ||
                                  length_of(confidence_values) < 2
                                  max_no > no+backtrack
                                -> "" max_no
                                :
                                  on use_colours: print! ansi_reset_colour()
                                  on be_verbose: print!
                                    dup("@bs; @bs;" length_of(next_piece))
                                  on use_colours: print! ansi_text_colour(BLACK)
                                  range &confidence_values 2 -1
                                  next!
                        -> response max_no
                -> "" max_no

  $print_piece: (piece confidence)
    on use_colours:
      $confidence_delta confidence-minimum_confidence
      $colour
        cond
          -> confidence_delta < 0 ->
            colour_mixture
              BLACK = -confidence_delta
              RED = minimum_confidence+confidence_delta
          -> confidence_delta > 10 ->
            colour_mixture
              GREEN = 10
              WHITE = confidence_delta-10
          -> true ->
            colour_mixture
              RED = 10-confidence_delta
              GREEN = confidence_delta
      print! ansi_background_colour(colour)
    on be_verbose: print! piece

$get_tokens: (model)
  $request get_tokens_request(model)
  open_tcp_client_socket! $fd address_of(model) port_no_of(model)
  print_to! fd request
  load! $result fd
  $error_code between(result ' ' ' ').to_integer
  if
    error_code >= 300:
      error result .behind. "@cr;@nl;@cr;@nl;"
    :
      behind &result "@cr;@nl;@cr;@nl;"
      from_utf8 &result
      $info result.from_json
      ->
        info("tokens")
        info("begin_of_stream")
        info("end_of_stream")
        info("classification")
        info("separator")
        info("newline")
        info("end_of_text")
        info("prefix")
        info("suffix")
        info("middle")
        info("context_size")

$get_tokens_request: (model)
  $options
    insert_order_table
      "model" = name_of(model)
      "uuid" = uuid_of(model)
  update_if ai::effective_context_size_of(model).is_defined &options
    -> options("context_size" ai::effective_context_size_of(model))
  $json options.to_json
  to_utf8 &json
  -> "
    POST /get_tokens HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)@

$ai_types::model/ai::tokenize:
  (
    model
    text
  )
  $request tokenize_request(model text)
  open_tcp_client_socket! $fd address_of(model) port_no_of(model)
  write_to! fd request
  load! $result fd
  behind &result "@cr;@nl;@cr;@nl;"
  from_utf8 &result
  $info result.from_json
  -> info("tokens")

$std_types::io/ai::tokenize:
  (
    io
    id
    model
    text
  )
  tuple &id 1
  run &io http_request id
    address_of(model) port_no_of(model) tokenize_request(model text)
  register_handlers io id
    JOB_COMPLETED = tokenize_completed
    JOB_FAILED = tokenize_failed

$tokenize_completed: (io id data)
  deregister_all_handlers &io id
  id !id
  from_utf8 &data
  from_json &data
  if
    data.is_an_error || data.is_undefined
    -> io tuple(JOB_FAILED id error(IO_ERROR "INVALID JSON"))
    -> io tuple(JOB_COMPLETED id data("tokens"))

$tokenize_failed: (io id err)
  deregister_all_handlers &io id
  id !id
  -> io tuple(JOB_FAILED id err)

$tokenize_request: (model text)
  $CONTENT
    if
      text.is_a_string
      -> "content"
      -> "lines"
  $json
    to_json
      insert_order_table
        "model" = name_of(model)
        "uuid" = uuid_of(model)
        CONTENT = text
  to_utf8 &json
  -> "
    POST /tokenize HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)@

$ai_types::model/ai::evaluate:
  (
    model
    prompt
    mode = ai::SMART
    start = 0
  )
  $do_return_confidence_values result_count() == 2
  $use_tokens not(prompt.is_a_string)
  $request evaluate_request(model prompt mode start)
  open_tcp_client_socket! $fd address_of(model) port_no_of(model)
  write_to! fd request
  load! $result fd
  behind &result "@cr;@nl;@cr;@nl;"
  from_utf8 &result
  $info result.from_json
  if
    do_return_confidence_values:
      $logits info("logits")
      map &logits: (logit) tuple logit(1) logit(2)
      if
        use_tokens
        -> info("token") logits
        -> info("content") logits
    :
      if
        use_tokens
        -> info("token")
        -> info("content")

$std_types::io/ai::evaluate:
  (
    io
    id
    model
    prompt
    mode = ai::SMART
  )
  tuple &id 1
  run &io http_request id
    address_of(model) port_no_of(model) evaluate_request(model prompt mode)
  register_handlers io id
    JOB_COMPLETED = evaluate_completed
    JOB_FAILED = evaluate_failed

$evaluate_completed: (io id data)
  deregister_all_handlers &io id
  id !id
  from_utf8 &data
  from_json &data
  if
    data.is_an_error || data.is_undefined
    -> io tuple(JOB_FAILED id error(IO_ERROR "INVALID JSON"))
    -> io tuple(JOB_COMPLETED id data("logits"))

$evaluate_failed: (io id err)
  deregister_all_handlers &io id
  id !id
  -> io tuple(JOB_FAILED id err)

$evaluate_request: (model prompt mode start = 0)
  $use_tokens not(prompt.is_a_string)
  $PROMPT
    if
      use_tokens
      -> "tokens"
      -> "prompt"
  case &mode
    ai::EXACT -> "exact"
    ai::SMART -> "smart"
    -> "exact"
  $json
    to_json
      insert_order_table
        PROMPT = prompt
        "start" = start
        "model" = name_of(model)
        "uuid" = uuid_of(model)
        "mode" = mode
        "logits" = true # also disables server side token selection
        "brief" = true
        "n_probs" = 10
        "n_predict" = 1
  to_utf8 &json
  -> "
    POST /completion HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)

$ai_types::model/ai::detokenize:
  (
    model
    token_or_list
    prefix = empty_list
  )
  $rc result_count()
  $pieces ai::pieces_of(model)
  if
    token_or_list.is_a_list:
      $text ""
      for_each token_or_list
        : (token)
          token_to_piece token $str &prefix
          append &text str
          next
        :
          if
            rc == 1
            -> text
            -> text prefix
    :
      token_to_piece token_or_list $str &prefix
      if
        rc == 1
        -> str
        -> str prefix

  $token_to_piece: (token prefix_octets)
    $piece pieces(token+1)
    if
      piece.is_a_list:
        append prefix_octets &piece
        $n length_of(piece)
        $i 1
        $e undefined
        loop:
          update_if i-1 <= n &e -> i-1
          if
            i > n:
              $octets ""
              for_each range(piece 1 e)
                : (code)
                  push &octets character(code)
                  next
                -> from_utf8(octets) range(piece e+1 -1)
            :
              $code piece(i)
              cond
                -> code < 0x80:
                  inc &i
                  next
                -> code & 0xe0 == 0xc0:
                  plus &i 2
                  next
                -> code & 0xf0 == 0xe0:
                  plus &i 3
                  next
                -> code & 0xf8 == 0xf0:
                  plus &i 4
                  next
                -> true -> "<???>" empty_list
      :
        update_if any_of(piece: (chr) -> chr == '@0x142;') &piece -> "<???>"
          # 'ł' cannot be printed - why?
        -> piece empty_list

$ai::connect_to_ai_server:
  (
    io
    id
    options*
  )
  update_if length_of(options) == 1 && options(1).is_a_list
    &options -> options(1)
  extract_options options
    ADDRESS = "127.0.0.1" $address
    PORT_NO = DEFAULT_PORT_NO $port_no
    UUID = undefined $uuid
  tuple &id 1
  run &io http_request id address port_no register_request(uuid)
  !io.connection_of(id)
    ai_types::connection
      .address_of address
      .port_no_of port_no
      .uuid_of uuid
  register_handlers io id
    JOB_COMPLETED = connect_completed
    JOB_FAILED = connect_failed

$connection_of ()

$std_types::io.connection_of empty_hash_table

$connect_completed: (io id _data)
  $connection connection_of(io)(id)
  !io.connection_of(id) undefined
  deregister_all_handlers &io id
  id !id
  -> io tuple(JOB_COMPLETED id connection)

$connect_failed: (io id err)
  !io.connection_of(id) undefined
  deregister_all_handlers &io id
  id !id
  -> io tuple(JOB_FAILED id err)

$register_request: (uuid)
  $json
    to_json
      insert_order_table
        "uuid" = uuid
  to_utf8 &json
  -> "
    POST /register HTTP/1.1@cr;
    Content-Type: application/json@cr;
    Content-Length: @(length_of(json))@cr;
    Connection: close@cr;
    @cr;
    @(json)@

[source]

$ai::evaluate ()

$ai_types::model/ai::evaluate:
  (
    model
    prompt
    mode = ai::SMART
    start = 0
  )
  $do_return_confidence_values result_count() == 2
  $use_tokens not(prompt.is_a_string)
  $request evaluate_request(model prompt mode start)
  open_tcp_client_socket! $fd address_of(model) port_no_of(model)
  write_to! fd request
  load! $result fd
  behind &result "@cr;@nl;@cr;@nl;"
  from_utf8 &result
  $info result.from_json
  if
    do_return_confidence_values:
      $logits info("logits")
      map &logits: (logit) tuple logit(1) logit(2)
      if
        use_tokens
        -> info("token") logits
        -> info("content") logits
    :
      if
        use_tokens
        -> info("token")
        -> info("content")

[documentation remark]

### Always copy examples contained in the symbol's description remark
    into the documentation

### Use only one of the following topics: AI, Input-Output

# ai_types::model/ai::evaluate (an I/O-method)

